# Pick model
RTNet_models <- c("01", "36")
mod <- RTNet_models[2]
# Pick threshold to filter
th <- 3
# Load datafiles
df1 <- read.csv(file.path("results", paste("model_",RTNet_models[1],"_evidence_level_sims.csv", sep=""))) %>%
mutate(model=RTNet_models[1]) %>%
filter(threshold==th) %>%
# Create column with binary confidence level
mutate(confidence_group=ifelse(confidence <= median(confidence), 0, 1))
df2 <- read.csv(file.path("results", paste("model_",RTNet_models[2],"_evidence_level_sims.csv", sep=""))) %>%
mutate(model=RTNet_models[2]) %>%
filter(threshold==th) %>%
# Create column with binary confidence level
mutate(confidence_group=ifelse(confidence <= median(confidence), 0, 1))
df <- rbind(df1, df2) %>%
# Factorize
mutate(model=factor(model)) %>%
mutate(choice=factor(choice)) %>%
mutate(true.label=factor(true.label)) %>%
# Create column with binary factor 'correct'
mutate(correct=true.label==choice)
# Get a sample to reduce computation time
data_sample <- df %>%
group_by(model) %>%
sample_n(size=20000)
#### Bayesian logistic regression using JAGS ####
modelString <- "model{
# Likelihood
for (i in 1:n){
y[i] ~ dbern(theta[i])
logit(theta[i]) <- b0+b1*x1[i]+b2*x2[i] + b3*(x1[i]*x2[i])
}
# prior
b0 ~ dnorm(0, 0.0001)
b1 ~ dnorm(0, 0.0001)
b2 ~ dnorm(0, 0.0001)
b3 ~ dnorm(0, 0.0001)
}"
model_path <- file.path("data", "JAGS", "log_reg_model_conf_levels.txt")
writeLines(modelString, model_path)
# Prepare data
data_sample <- data_sample %>%
filter(model==mod) # Filter by model, adjust as necessary
# Prepare datalist for JAGS
dataList <- list(y=as.numeric(data_sample$correct),
x1=data_sample$evidence,
x2=data_sample$confidence_group,
n=nrow(data_sample))
# Prepare model
# Model parameters
pars <- c("b0", "b1", "b2")
nChains <- 3
inits <- list(
list("b0" = rnorm(1, 0, 10), "b1" = rnorm(1, 0, 10), "b2" = rnorm(1, 0, 10)),  # Chain 1
list("b0" = rnorm(1, 0, 10), "b1" = rnorm(1, 0, 10), "b2" = rnorm(1, 0, 10)),  # Chain 2
list("b0" = rnorm(1, 0, 10), "b1" = rnorm(1, 0, 10), "b2" = rnorm(1, 0, 10))   # Chain 3
)
samples <- 8000
burnin <- 1000
nAdapt <- 1000
# Path to model (use either for saving or loading)
jags_model_path <- file.path("data", "JAGS", paste("model_",mod,"_jags_output_th_", th, ".RData", sep=""))
# Run model or load previous model
set.seed(42) # Set seed
output <- run.jags(model=model_path,
monitor=pars,
data=dataList,
n.chains = nChains,
inits=inits,
burnin=burnin,
sample=samples,
adapt=nAdapt,
method="rjparallel"
)
# Save the output (to prevent long sampling time of about 10 minutes)
save(output, file=jags_model_path)
# MCMC diagnostics
mcmc_trace(output$mcmc, pars) # traceplot
mcmc_acf(output$mcmc, pars) # autocorrelation
library(dplyr)
library(papaja)
library(ggridges)
library(ggplot2)
library(gridExtra)
library(viridis)
library(runjags)
library(bayesplot)
#### Data preparation ####
# Pick model
RTNet_models <- c("01", "36")
mod <- RTNet_models[2]
# Pick threshold to filter
th <- 3
# Load datafiles
df1 <- read.csv(file.path("results", paste("model_",RTNet_models[1],"_evidence_level_sims.csv", sep=""))) %>%
mutate(model=RTNet_models[1]) %>%
filter(threshold==th) %>%
# Create column with binary confidence level
mutate(confidence_group=ifelse(confidence <= median(confidence), 0, 1))
df2 <- read.csv(file.path("results", paste("model_",RTNet_models[2],"_evidence_level_sims.csv", sep=""))) %>%
mutate(model=RTNet_models[2]) %>%
filter(threshold==th) %>%
# Create column with binary confidence level
mutate(confidence_group=ifelse(confidence <= median(confidence), 0, 1))
df <- rbind(df1, df2) %>%
# Factorize
mutate(model=factor(model)) %>%
mutate(choice=factor(choice)) %>%
mutate(true.label=factor(true.label)) %>%
# Create column with binary factor 'correct'
mutate(correct=true.label==choice)
# Get a sample to reduce computation time
data_sample <- df %>%
group_by(model) %>%
sample_n(size=20000)
#### Bayesian logistic regression using JAGS ####
modelString <- "model{
# Likelihood
for (i in 1:n){
y[i] ~ dbern(theta[i])
logit(theta[i]) <- b0+b1*x1[i]+b2*x2[i] + b3*(x1[i]*x2[i])
}
# prior
b0 ~ dnorm(0, 0.0001)
b1 ~ dnorm(0, 0.0001)
b2 ~ dnorm(0, 0.0001)
b3 ~ dnorm(0, 0.0001)
}"
model_path <- file.path("data", "JAGS", "log_reg_model_conf_levels.txt")
writeLines(modelString, model_path)
# Prepare data
data_sample <- data_sample %>%
filter(model==mod) # Filter by model, adjust as necessary
# Prepare datalist for JAGS
dataList <- list(y=as.numeric(data_sample$correct),
x1=data_sample$evidence,
x2=data_sample$confidence_group,
n=nrow(data_sample))
# Prepare model
# Model parameters
pars <- c("b0", "b1", "b2", "b3")
nChains <- 3
inits <- list(
list("b0" = rnorm(1, 0, 10), "b1" = rnorm(1, 0, 10), "b2" = rnorm(1, 0, 10), "b3" = rnorm(1,0,10)),  # Chain 1
list("b0" = rnorm(1, 0, 10), "b1" = rnorm(1, 0, 10), "b2" = rnorm(1, 0, 10), "b3" = rnorm(1,0,10)),  # Chain 2
list("b0" = rnorm(1, 0, 10), "b1" = rnorm(1, 0, 10), "b2" = rnorm(1, 0, 10), "b3" = rnorm(1,0,10))   # Chain 3
)
samples <- 8000
burnin <- 1000
nAdapt <- 1000
# Path to model (use either for saving or loading)
jags_model_path <- file.path("data", "JAGS", paste("model_",mod,"_jags_output_th_", th, ".RData", sep=""))
# Run model or load previous model
set.seed(42) # Set seed
output <- run.jags(model=model_path,
monitor=pars,
data=dataList,
n.chains = nChains,
inits=inits,
burnin=burnin,
sample=samples,
adapt=nAdapt,
method="rjparallel"
)
# Save the output (to prevent long sampling time of about 10 minutes)
save(output, file=jags_model_path)
# MCMC diagnostics
mcmc_trace(output$mcmc, pars) # traceplot
mcmc_acf(output$mcmc, pars) # autocorrelation
# Path to model for loading
# Pick model
mod <- RTNet_models[1]
jags_model_path <- file.path("data", "JAGS", paste("model_",mod,"_jags_output_th_", th, ".RData", sep=""))
# load model
load(jags_model_path)
# Extract parameter samples from posterior
param_samples <- combine.mcmc(output)
b0 <- as.numeric(param_samples[,1])
b1 <- as.numeric(param_samples[,2])
b2 <- as.numeric(param_samples[,3])
b3 <- as.numeric(param_samples[,4])
head(param_samples)
# Path to model for loading
# Pick model
mod <- RTNet_models[2]
jags_model_path <- file.path("data", "JAGS", paste("model_",mod,"_jags_output_th_", th, ".RData", sep=""))
# load model
load(jags_model_path)
# Extract parameter samples from posterior
param_samples <- combine.mcmc(output)
b0 <- as.numeric(param_samples[,1])
b1 <- as.numeric(param_samples[,2])
b2 <- as.numeric(param_samples[,3])
b3 <- as.numeric(param_samples[,4])
# Means
b0_m <- mean(b0)
b1_m <- mean(b1)
b2_m <- mean(b2)
b3_m <- mean(b3)
# filter data by model
acc_data <- data_summary$by_confidence %>%
filter(model==mod)
# Confidence group specific accuracies
acc_low_conf <- acc_data[acc_data$confidence_group=="low",]
acc_high_conf <- acc_data[acc_data$confidence_group=="high",]
# Plot the Bayesian regression lines
plot(x=acc_low_conf$evidence, xlab= "Evidence level",
y=acc_low_conf$acc, ylab = "Accuracy", cex=1, pch=16,
col='darkblue', ylim=c(0,1))
# add title
title(paste("Model", mod))
# Add high confidence curve
points(acc_high_conf$evidence, acc_high_conf$acc, col="red", pch=16)
# Plot sample of regression lines
smp <- sample(nrow(param_samples), 500)
# Curve for low confidence
invisible(lapply(smp, function(i) curve(exp(b0[i] + b1[i]*x)/(1+exp(b0[i] + b1[i]*x)), col=adjustcolor("purple",alpha=.1), add=TRUE )))
# Curve for high confidence
invisible(lapply(smp, function(i) curve(exp(b0[i] + b2[i] +b1[i]*x + b3[i]*x)/(1+exp(b0[i] + b2[i] + b1[i]*x + b3[i]*x)), col=adjustcolor("orange",alpha=.1), add=TRUE )))
# Mean curve for low confidence
curve(exp(b0_m+b1_m*x)/(1+exp(b0_m+b1_m*x)), col="darkblue", add=TRUE, lwd=2)
# Mean curve for high confidence
curve(exp(b0_m+b2_m+b1_m*x+b3_m*x)/(1+exp(b0_m+b2_m+b1_m*x+b3_m*x)), col="red", add=TRUE, lwd=2)
# Add a legend to the plot
legend("bottomright",
legend=c("High confidence (mean)", "Low confidence (mean)", "High confidence (uncertainty)", "Low confidence (uncertainty)","High confidence (data)", "Low confidence (data)"),
col=c("red", "darkblue", adjustcolor("orange", alpha=.5), adjustcolor("purple", alpha=.5), 'red', "darkblue"),
lwd=c(2, 2, 1, 1, NA, NA),
pch=c(NA, NA, NA, NA, 16, 16),
bty="n",
cex=0.8)
library(dplyr)
library(papaja)
library(ggridges)
library(ggplot2)
library(gridExtra)
library(viridis)
library(runjags)
library(bayesplot)
#### Data preparation ####
# Pick model
RTNet_models <- c("01", "36")
mod <- RTNet_models[1]
# Pick threshold to filter
th <- 3
# Load datafiles
df1 <- read.csv(file.path("results", paste("model_",RTNet_models[1],"_evidence_level_sims.csv", sep=""))) %>%
mutate(model=RTNet_models[1]) %>%
filter(threshold==th) %>%
# Create column with binary confidence level
mutate(confidence_group=ifelse(confidence <= median(confidence), 0, 1))
df2 <- read.csv(file.path("results", paste("model_",RTNet_models[2],"_evidence_level_sims.csv", sep=""))) %>%
mutate(model=RTNet_models[2]) %>%
filter(threshold==th) %>%
# Create column with binary confidence level
mutate(confidence_group=ifelse(confidence <= median(confidence), 0, 1))
df <- rbind(df1, df2) %>%
# Factorize
mutate(model=factor(model)) %>%
mutate(choice=factor(choice)) %>%
mutate(true.label=factor(true.label)) %>%
# Create column with binary factor 'correct'
mutate(correct=true.label==choice)
# Get a sample to reduce computation time
data_sample <- df %>%
group_by(model) %>%
sample_n(size=20000)
#### Bayesian logistic regression using JAGS ####
modelString <- "model{
# Likelihood
for (i in 1:n){
y[i] ~ dbern(theta[i])
logit(theta[i]) <- b0+b1*x1[i]+b2*x2[i] + b3*(x1[i]*x2[i])
}
# prior
b0 ~ dnorm(0, 0.0001)
b1 ~ dnorm(0, 0.0001)
b2 ~ dnorm(0, 0.0001)
b3 ~ dnorm(0, 0.0001)
}"
model_path <- file.path("data", "JAGS", "log_reg_model_conf_levels.txt")
writeLines(modelString, model_path)
# Prepare data
data_sample <- data_sample %>%
filter(model==mod) # Filter by model, adjust as necessary
# Prepare datalist for JAGS
dataList <- list(y=as.numeric(data_sample$correct),
x1=data_sample$evidence,
x2=data_sample$confidence_group,
n=nrow(data_sample))
# Prepare model
# Model parameters
pars <- c("b0", "b1", "b2", "b3")
nChains <- 3
inits <- list(
list("b0" = rnorm(1, 0, 10), "b1" = rnorm(1, 0, 10), "b2" = rnorm(1, 0, 10), "b3" = rnorm(1,0,10)),  # Chain 1
list("b0" = rnorm(1, 0, 10), "b1" = rnorm(1, 0, 10), "b2" = rnorm(1, 0, 10), "b3" = rnorm(1,0,10)),  # Chain 2
list("b0" = rnorm(1, 0, 10), "b1" = rnorm(1, 0, 10), "b2" = rnorm(1, 0, 10), "b3" = rnorm(1,0,10))   # Chain 3
)
samples <- 8000
burnin <- 1000
nAdapt <- 1000
# Path to model (use either for saving or loading)
jags_model_path <- file.path("data", "JAGS", paste("model_",mod,"_jags_output_th_", th, ".RData", sep=""))
# Run model or load previous model
set.seed(42) # Set seed
output <- run.jags(model=model_path,
monitor=pars,
data=dataList,
n.chains = nChains,
inits=inits,
burnin=burnin,
sample=samples,
adapt=nAdapt,
method="rjparallel"
)
# Save the output (to prevent long sampling time of about 10 minutes)
save(output, file=jags_model_path)
# MCMC diagnostics
mcmc_trace(output$mcmc, pars) # traceplot
mcmc_acf(output$mcmc, pars) # autocorrelation
# Path to model for loading
# Pick model
mod <- RTNet_models[1]
jags_model_path <- file.path("data", "JAGS", paste("model_",mod,"_jags_output_th_", th, ".RData", sep=""))
# load model
load(jags_model_path)
# Extract parameter samples from posterior
param_samples <- combine.mcmc(output)
b0 <- as.numeric(param_samples[,1])
b1 <- as.numeric(param_samples[,2])
b2 <- as.numeric(param_samples[,3])
b3 <- as.numeric(param_samples[,4])
# Means
b0_m <- mean(b0)
b1_m <- mean(b1)
b2_m <- mean(b2)
b3_m <- mean(b3)
# filter data by model
acc_data <- data_summary$by_confidence %>%
filter(model==mod)
# Confidence group specific accuracies
acc_low_conf <- acc_data[acc_data$confidence_group=="low",]
acc_high_conf <- acc_data[acc_data$confidence_group=="high",]
# Plot the Bayesian regression lines
plot(x=acc_low_conf$evidence, xlab= "Evidence level",
y=acc_low_conf$acc, ylab = "Accuracy", cex=1, pch=16,
col='darkblue', ylim=c(0,1))
# add title
title(paste("Model", mod))
# Add high confidence curve
points(acc_high_conf$evidence, acc_high_conf$acc, col="red", pch=16)
# Plot sample of regression lines
smp <- sample(nrow(param_samples), 500)
# Curve for low confidence
invisible(lapply(smp, function(i) curve(exp(b0[i] + b1[i]*x)/(1+exp(b0[i] + b1[i]*x)), col=adjustcolor("purple",alpha=.1), add=TRUE )))
# Curve for high confidence
invisible(lapply(smp, function(i) curve(exp(b0[i] + b2[i] +b1[i]*x + b3[i]*x)/(1+exp(b0[i] + b2[i] + b1[i]*x + b3[i]*x)), col=adjustcolor("orange",alpha=.1), add=TRUE )))
# Mean curve for low confidence
curve(exp(b0_m+b1_m*x)/(1+exp(b0_m+b1_m*x)), col="darkblue", add=TRUE, lwd=2)
# Mean curve for high confidence
curve(exp(b0_m+b2_m+b1_m*x+b3_m*x)/(1+exp(b0_m+b2_m+b1_m*x+b3_m*x)), col="red", add=TRUE, lwd=2)
# Add a legend to the plot
legend("bottomright",
legend=c("High confidence (mean)", "Low confidence (mean)", "High confidence (uncertainty)", "Low confidence (uncertainty)","High confidence (data)", "Low confidence (data)"),
col=c("red", "darkblue", adjustcolor("orange", alpha=.5), adjustcolor("purple", alpha=.5), 'red', "darkblue"),
lwd=c(2, 2, 1, 1, NA, NA),
pch=c(NA, NA, NA, NA, 16, 16),
bty="n",
cex=0.8)
library(dplyr)
library(papaja)
library(ggridges)
library(ggplot2)
library(gridExtra)
library(viridis)
library(runjags)
library(bayesplot)
#### Data preparation ####
# Pick model
RTNet_models <- c("01", "36")
mod <- RTNet_models[1]
# Pick threshold to filter
th <- 3
# Load datafiles
df1 <- read.csv(file.path("results", paste("model_",RTNet_models[1],"_evidence_level_sims.csv", sep=""))) %>%
mutate(model=RTNet_models[1]) %>%
filter(threshold==th) %>%
# Create column with binary confidence level
mutate(confidence_group=ifelse(confidence <= median(confidence), 0, 1))
df2 <- read.csv(file.path("results", paste("model_",RTNet_models[2],"_evidence_level_sims.csv", sep=""))) %>%
mutate(model=RTNet_models[2]) %>%
filter(threshold==th) %>%
# Create column with binary confidence level
mutate(confidence_group=ifelse(confidence <= median(confidence), 0, 1))
df <- rbind(df1, df2) %>%
# Factorize
mutate(model=factor(model)) %>%
mutate(choice=factor(choice)) %>%
mutate(true.label=factor(true.label)) %>%
# Create column with binary factor 'correct'
mutate(correct=true.label==choice)
# Get a sample to reduce computation time
data_sample <- df %>%
group_by(model) %>%
sample_n(size=20000)
#### Bayesian logistic regression using JAGS ####
modelString <- "model{
# Likelihood
for (i in 1:n){
y[i] ~ dbern(theta[i])
logit(theta[i]) <- b0+b1*x1[i]+b2*x2[i] + b3*(x1[i]*x2[i])
}
# prior
b0 ~ dnorm(0, 0.0001)
b1 ~ dnorm(0, 0.0001)
b2 ~ dnorm(0, 0.0001)
b3 ~ dnorm(0, 0.0001)
}"
model_path <- file.path("data", "JAGS", "log_reg_model_conf_levels.txt")
writeLines(modelString, model_path)
# Prepare data
data_sample <- data_sample %>%
filter(model==mod) # Filter by model, adjust as necessary
# Prepare datalist for JAGS
dataList <- list(y=as.numeric(data_sample$correct),
x1=data_sample$evidence,
x2=data_sample$confidence_group,
n=nrow(data_sample))
# Prepare model
# Model parameters
pars <- c("b0", "b1", "b2", "b3")
nChains <- 3
inits <- list(
list("b0" = rnorm(1, 0, 10), "b1" = rnorm(1, 0, 10), "b2" = rnorm(1, 0, 10), "b3" = rnorm(1,0,10)),  # Chain 1
list("b0" = rnorm(1, 0, 10), "b1" = rnorm(1, 0, 10), "b2" = rnorm(1, 0, 10), "b3" = rnorm(1,0,10)),  # Chain 2
list("b0" = rnorm(1, 0, 10), "b1" = rnorm(1, 0, 10), "b2" = rnorm(1, 0, 10), "b3" = rnorm(1,0,10))   # Chain 3
)
samples <- 8000
burnin <- 1000
nAdapt <- 1000
# Path to model (use either for saving or loading)
jags_model_path <- file.path("data", "JAGS", paste("model_",mod,"_jags_output_th_", th, ".RData", sep=""))
# Run model or load previous model
set.seed(42) # Set seed
output <- run.jags(model=model_path,
monitor=pars,
data=dataList,
n.chains = nChains,
inits=inits,
burnin=burnin,
sample=samples,
adapt=nAdapt,
method="rjparallel"
)
# Save the output (to prevent long sampling time of about 10 minutes)
save(output, file=jags_model_path)
# MCMC diagnostics
mcmc_trace(output$mcmc, pars) # traceplot
mcmc_acf(output$mcmc, pars) # autocorrelation
output$timetaken
1011.849/60
# Pick model
mod <- RTNet_models[1]
jags_model_path <- file.path("data", "JAGS", paste("model_",mod,"_jags_output_th_", th, ".RData", sep=""))
# load model
load(jags_model_path)
# Extract parameter samples from posterior
param_samples <- combine.mcmc(output)
b0 <- as.numeric(param_samples[,1])
b1 <- as.numeric(param_samples[,2])
b2 <- as.numeric(param_samples[,3])
b3 <- as.numeric(param_samples[,4])
# Means
b0_m <- mean(b0)
b1_m <- mean(b1)
b2_m <- mean(b2)
b3_m <- mean(b3)
# filter data by model
acc_data <- data_summary$by_confidence %>%
filter(model==mod)
# Confidence group specific accuracies
acc_low_conf <- acc_data[acc_data$confidence_group=="low",]
acc_high_conf <- acc_data[acc_data$confidence_group=="high",]
# Plot the Bayesian regression lines
plot(x=acc_low_conf$evidence, xlab= "Evidence level",
y=acc_low_conf$acc, ylab = "Accuracy", cex=1, pch=16,
col='darkblue', ylim=c(0,1))
# add title
title(paste("Model", mod))
# Add high confidence curve
points(acc_high_conf$evidence, acc_high_conf$acc, col="red", pch=16)
# Plot sample of regression lines
smp <- sample(nrow(param_samples), 500)
# Curve for low confidence
invisible(lapply(smp, function(i) curve(exp(b0[i] + b1[i]*x)/(1+exp(b0[i] + b1[i]*x)), col=adjustcolor("purple",alpha=.1), add=TRUE )))
# Curve for high confidence
invisible(lapply(smp, function(i) curve(exp(b0[i] + b2[i] +b1[i]*x + b3[i]*x)/(1+exp(b0[i] + b2[i] + b1[i]*x + b3[i]*x)), col=adjustcolor("orange",alpha=.1), add=TRUE )))
# Mean curve for low confidence
curve(exp(b0_m+b1_m*x)/(1+exp(b0_m+b1_m*x)), col="darkblue", add=TRUE, lwd=2)
# Mean curve for high confidence
curve(exp(b0_m+b2_m+b1_m*x+b3_m*x)/(1+exp(b0_m+b2_m+b1_m*x+b3_m*x)), col="red", add=TRUE, lwd=2)
# Add a legend to the plot
legend("bottomright",
legend=c("High confidence (mean)", "Low confidence (mean)", "High confidence (uncertainty)", "Low confidence (uncertainty)","High confidence (data)", "Low confidence (data)"),
col=c("red", "darkblue", adjustcolor("orange", alpha=.5), adjustcolor("purple", alpha=.5), 'red', "darkblue"),
lwd=c(2, 2, 1, 1, NA, NA),
pch=c(NA, NA, NA, NA, 16, 16),
bty="n",
cex=0.8)

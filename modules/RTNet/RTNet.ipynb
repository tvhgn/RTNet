{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlSS96RwlFrT"
      },
      "source": [
        "**How to run this script?**\n",
        "\n",
        "\n",
        "1.   Navigate to \"**Load the pretrained model**\" section and set the path\n",
        "where you located the Bayesian models.\n",
        "2.   Navigate to \"**Simulations**\" section and provide a location where you want to save the model output.\n",
        "3. You are all set! Now you can run the script on Google Colab.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4JAgS4o3mg5",
        "outputId": "a3a366cc-40f2-469c-ad66-63b215eca3d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyro-ppl==0.2.1 in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl==0.2.1) (21.6.0)\n",
            "Requirement already satisfied: graphviz>=0.8 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl==0.2.1) (0.20.3)\n",
            "Requirement already satisfied: networkx>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl==0.2.1) (3.4.2)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl==0.2.1) (1.26.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl==0.2.1) (1.16.0)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl==0.2.1) (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->pyro-ppl==0.2.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->pyro-ppl==0.2.1) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->pyro-ppl==0.2.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->pyro-ppl==0.2.1) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->pyro-ppl==0.2.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=0.4.0->pyro-ppl==0.2.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->pyro-ppl==0.2.1) (3.0.2)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Switch\n",
        "local = False\n",
        "\n",
        "if local:\n",
        "    mnist_data_path = os.path.join(\"..\", \"..\", \"data\", \"mnist-data\")\n",
        "\n",
        "    # Set paths\n",
        "    model_num = \"01\"\n",
        "    model_path = os.path.join(\"..\", \"..\", \"data\", \"Bayesian_models\")\n",
        "    # Path for saving results\n",
        "    save_dir = os.path.join(\"..\", \"..\", \"results\")\n",
        "\n",
        "else: # We're on colab\n",
        "    from google.colab import drive\n",
        "    # Install pyro package\n",
        "    !pip3 install pyro-ppl==0.2.1\n",
        "    # Set different mnist path\n",
        "    mnist_data_path = os.path.join(\"mnist-data\")\n",
        "\n",
        "    # Mount google drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Set paths\n",
        "    model_num = \"01\"\n",
        "    model_path = os.path.join(\"drive\", \"MyDrive\", \"RTNet\", \"models\")\n",
        "    # Path for saving results\n",
        "    save_dir = os.path.join(\"drive\", \"MyDrive\", \"RTNet\", \"results\")\n",
        "\n",
        "# Check whether folders exist\n",
        "assert os.path.exists(model_path)\n",
        "assert os.path.exists(save_dir)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "N6jMxsF_xVp4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import loadmat\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import pyro\n",
        "from pyro.distributions import Normal, Categorical\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.optim import Adam\n",
        "\n",
        "#from google.colab import files\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rmEFaPL30Sh",
        "outputId": "004fe927-44fd-4cc7-a3cc-e98f655e1033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Check Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0DJT0xeAqXL"
      },
      "source": [
        "**Load the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTRmzMB4yMxj",
        "outputId": "3d659062-7d05-43e7-83ee-49f1e1c5c17e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to mnist-data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 15.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist-data/MNIST/raw/train-images-idx3-ubyte.gz to mnist-data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to mnist-data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 496kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist-data/MNIST/raw/train-labels-idx1-ubyte.gz to mnist-data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to mnist-data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.88MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist-data/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist-data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to mnist-data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 9.03MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist-data/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist-data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "AlexTransform = transforms.Compose([\n",
        "    transforms.Resize((227, 227)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST(mnist_data_path, train=True, download=True, transform=AlexTransform),\n",
        "        batch_size=500, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST(mnist_data_path, train=False, download=True, transform=AlexTransform),\n",
        "        batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Qnn6wDRi8zh0"
      },
      "outputs": [],
      "source": [
        "# # View images\n",
        "# complete_im = []\n",
        "# images_high_conf = [5283, 5726, 1099, 792, 8199, 6817, 3376, 9530, 1150, 3497]\n",
        "# images_low_conf = [7736, 8151, 7701, 8537, 9359, 6520, 4316, 1452, 3699, 9261]\n",
        "# for idx in images_high_conf:\n",
        "#     im, _ = test_loader.dataset[idx]\n",
        "#     complete_im.append(im)\n",
        "\n",
        "# images = torchvision.utils.make_grid(complete_im)\n",
        "\n",
        "# img = torchvision.transforms.ToPILImage()(images)\n",
        "# img.show()\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyqaSFcs3Udj"
      },
      "source": [
        "**Define the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4QqsElxSF2F"
      },
      "source": [
        "AlexNet structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XNBayOSDyyn9"
      },
      "outputs": [],
      "source": [
        "# AlexNet\n",
        "class alexnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=96, kernel_size=11, stride=4, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(96, 256, 5, 1, 2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 384, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(384, 384, 3, 1, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(384, 256, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2)\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fc3 = nn.Linear(4096, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.conv5(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "\n",
        "        out = F.relu(self.fc1(out))  # 256*6*6 -> 4096\n",
        "        out = F.dropout(out, 0.5)\n",
        "        out = F.relu(self.fc2(out))\n",
        "        out = F.dropout(out, 0.5)\n",
        "        out = self.fc3(out)\n",
        "        # out = F.log_softmax(out, dim=1)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "l-94O5iW367h"
      },
      "outputs": [],
      "source": [
        "# Define Hyper parameters\n",
        "img_size = 28 * 28\n",
        "hidden_layer_size = 1024\n",
        "num_classes = 10\n",
        "net = alexnet().to(device)\n",
        "# softmax\n",
        "log_softmax = nn.LogSoftmax(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COLZgZNISK4g"
      },
      "source": [
        "Model function for pyro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "liRRzYaD4Ejg"
      },
      "outputs": [],
      "source": [
        "def model(x_data, y_data):\n",
        "\n",
        "    convLayer1_w = Normal(loc=torch.ones_like(net.conv1[0].weight), scale=torch.ones_like(net.conv1[0].weight))\n",
        "    convLayer1_b = Normal(loc=torch.ones_like(net.conv1[0].bias), scale=torch.ones_like(net.conv1[0].bias))\n",
        "\n",
        "    convLayer2_w = Normal(loc=torch.ones_like(net.conv2[0].weight), scale=torch.ones_like(net.conv2[0].weight))\n",
        "    convLayer2_b = Normal(loc=torch.ones_like(net.conv2[0].bias), scale=torch.ones_like(net.conv2[0].bias))\n",
        "\n",
        "    convLayer3_w = Normal(loc=torch.ones_like(net.conv3[0].weight), scale=torch.ones_like(net.conv3[0].weight))\n",
        "    convLayer3_b = Normal(loc=torch.ones_like(net.conv3[0].bias), scale=torch.ones_like(net.conv3[0].bias))\n",
        "\n",
        "    convLayer4_w = Normal(loc=torch.ones_like(net.conv4[0].weight), scale=torch.ones_like(net.conv4[0].weight))\n",
        "    convLayer4_b = Normal(loc=torch.ones_like(net.conv4[0].bias), scale=torch.ones_like(net.conv4[0].bias))\n",
        "\n",
        "    convLayer5_w = Normal(loc=torch.ones_like(net.conv5[0].weight), scale=torch.ones_like(net.conv5[0].weight))\n",
        "    convLayer5_b = Normal(loc=torch.ones_like(net.conv5[0].bias), scale=torch.ones_like(net.conv5[0].bias))\n",
        "\n",
        "    fc1Layer_w = Normal(loc=torch.ones_like(net.fc1.weight), scale=torch.ones_like(net.fc1.weight))\n",
        "    fc1Layer_b = Normal(loc=torch.ones_like(net.fc1.bias), scale=torch.ones_like(net.fc1.bias))\n",
        "\n",
        "    fc2Layer_w = Normal(loc=torch.ones_like(net.fc2.weight), scale=torch.ones_like(net.fc2.weight))\n",
        "    fc2Layer_b = Normal(loc=torch.ones_like(net.fc2.bias), scale=torch.ones_like(net.fc2.bias))\n",
        "\n",
        "    fc3Layer_w = Normal(loc=torch.ones_like(net.fc3.weight), scale=torch.ones_like(net.fc3.weight))\n",
        "    fc3Layer_b = Normal(loc=torch.ones_like(net.fc3.bias), scale=torch.ones_like(net.fc3.bias))\n",
        "\n",
        "    priors = {'conv1[0].weight': convLayer1_w,\n",
        "              'conv1[0].bias': convLayer1_b,\n",
        "              'conv2[0].weight': convLayer2_w,\n",
        "              'conv2[0].bias': convLayer2_b,\n",
        "              'conv3[0].weight': convLayer3_w,\n",
        "              'conv3[0].bias': convLayer3_b,\n",
        "              'conv4[0].weight': convLayer4_w,\n",
        "              'conv4[0].bias': convLayer4_b,\n",
        "              'conv5[0].weight': convLayer5_w,\n",
        "              'conv5[0].bias': convLayer5_b,\n",
        "              'fc1.weight': fc1Layer_w,\n",
        "              'fc1.bias': fc1Layer_b,\n",
        "              'fc2.weight': fc2Layer_w,\n",
        "              'fc2.bias': fc2Layer_b,\n",
        "              'fc3.weight': fc3Layer_w,\n",
        "              'fc3.bias': fc3Layer_b}\n",
        "\n",
        "    # lift module parameters to random variables sampled from the priors\n",
        "    lifted_module = pyro.random_module(\"module\", net, priors)\n",
        "    # sample a regressor (which also samples w and b)\n",
        "    lifted_reg_model = lifted_module()\n",
        "\n",
        "    lhat = log_softmax(lifted_reg_model(x_data))\n",
        "\n",
        "    pyro.sample(\"obs\", Categorical(logits=lhat), obs=y_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX4OnUndST19"
      },
      "source": [
        "Guide function for pyro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "So18-_z_4Hv3"
      },
      "outputs": [],
      "source": [
        "softplus = torch.nn.Softplus()\n",
        "\n",
        "def guide(x_data, y_data):\n",
        "\n",
        "    # First layer weight distribution priors\n",
        "    convLayer1w_mu    = torch.randn_like(net.conv1[0].weight)\n",
        "    convLayer1w_sigma = torch.randn_like(net.conv1[0].weight)\n",
        "    convLayer1w_mu_param    = pyro.param(\"convLayer1w_mu\", convLayer1w_mu)\n",
        "    convLayer1w_sigma_param = softplus(pyro.param(\"convLayer1w_sigma\", convLayer1w_sigma))\n",
        "    convLayer1_w = Normal(loc=convLayer1w_mu_param, scale=convLayer1w_sigma_param)\n",
        "\n",
        "    # First layer bias distribution priors\n",
        "    convLayer1b_mu    = torch.randn_like(net.conv1[0].bias)\n",
        "    convLayer1b_sigma = torch.randn_like(net.conv1[0].bias)\n",
        "    convLayer1b_mu_param    = pyro.param(\"convLayer1b_mu\", convLayer1b_mu)\n",
        "    convLayer1b_sigma_param = softplus(pyro.param(\"convLayer1b_sigma\", convLayer1b_sigma))\n",
        "    convLayer1_b = Normal(loc=convLayer1b_mu_param, scale=convLayer1b_sigma_param)\n",
        "\n",
        "    # Second layer weight distribution priors\n",
        "    convLayer2w_mu    = torch.randn_like(net.conv2[0].weight)\n",
        "    convLayer2w_sigma = torch.randn_like(net.conv2[0].weight)\n",
        "    convLayer2w_mu_param    = pyro.param(\"convLayer2w_mu\", convLayer2w_mu)\n",
        "    convLayer2w_sigma_param = softplus(pyro.param(\"convLayer2w_sigma\", convLayer2w_sigma))\n",
        "    convLayer2_w = Normal(loc=convLayer2w_mu_param, scale=convLayer2w_sigma_param)\n",
        "\n",
        "    # Second layer bias distribution priors\n",
        "    convLayer2b_mu    = torch.randn_like(net.conv2[0].bias)\n",
        "    convLayer2b_sigma = torch.randn_like(net.conv2[0].bias)\n",
        "    convLayer2b_mu_param    = pyro.param(\"convLayer2b_mu\", convLayer2b_mu)\n",
        "    convLayer2b_sigma_param = softplus(pyro.param(\"convLayer2b_sigma\", convLayer2b_sigma))\n",
        "    convLayer2_b = Normal(loc=convLayer2b_mu_param, scale=convLayer2b_sigma_param)\n",
        "\n",
        "    # Third layer weight distribution priors\n",
        "    convLayer3w_mu    = torch.randn_like(net.conv3[0].weight)\n",
        "    convLayer3w_sigma = torch.randn_like(net.conv3[0].weight)\n",
        "    convLayer3w_mu_param    = pyro.param(\"convLayer3w_mu\", convLayer3w_mu)\n",
        "    convLayer3w_sigma_param = softplus(pyro.param(\"convLayer3w_sigma\", convLayer3w_sigma))\n",
        "    convLayer3_w = Normal(loc=convLayer3w_mu_param, scale=convLayer3w_sigma_param)\n",
        "\n",
        "    # Third layer bias distribution priors\n",
        "    convLayer3b_mu    = torch.randn_like(net.conv3[0].bias)\n",
        "    convLayer3b_sigma = torch.randn_like(net.conv3[0].bias)\n",
        "    convLayer3b_mu_param    = pyro.param(\"convLayer3b_mu\", convLayer3b_mu)\n",
        "    convLayer3b_sigma_param = softplus(pyro.param(\"convLayer3b_sigma\", convLayer3b_sigma))\n",
        "    convLayer3_b = Normal(loc=convLayer3b_mu_param, scale=convLayer3b_sigma_param)\n",
        "\n",
        "    # Fourth layer weight distribution priors\n",
        "    convLayer4w_mu    = torch.randn_like(net.conv4[0].weight)\n",
        "    convLayer4w_sigma = torch.randn_like(net.conv4[0].weight)\n",
        "    convLayer4w_mu_param    = pyro.param(\"convLayer4w_mu\", convLayer4w_mu)\n",
        "    convLayer4w_sigma_param = softplus(pyro.param(\"convLayer4w_sigma\", convLayer4w_sigma))\n",
        "    convLayer4_w = Normal(loc=convLayer4w_mu_param, scale=convLayer4w_sigma_param)\n",
        "\n",
        "    # Fourth layer bias distribution priors\n",
        "    convLayer4b_mu    = torch.randn_like(net.conv4[0].bias)\n",
        "    convLayer4b_sigma = torch.randn_like(net.conv4[0].bias)\n",
        "    convLayer4b_mu_param    = pyro.param(\"convLayer4b_mu\", convLayer4b_mu)\n",
        "    convLayer4b_sigma_param = softplus(pyro.param(\"convLayer4b_sigma\", convLayer4b_sigma))\n",
        "    convLayer4_b = Normal(loc=convLayer4b_mu_param, scale=convLayer4b_sigma_param)\n",
        "\n",
        "    # Fifth layer weight distribution priors\n",
        "    convLayer5w_mu    = torch.randn_like(net.conv5[0].weight)\n",
        "    convLayer5w_sigma = torch.randn_like(net.conv5[0].weight)\n",
        "    convLayer5w_mu_param    = pyro.param(\"convLayer5w_mu\", convLayer5w_mu)\n",
        "    convLayer5w_sigma_param = softplus(pyro.param(\"convLayer5w_sigma\", convLayer5w_sigma))\n",
        "    convLayer5_w = Normal(loc=convLayer5w_mu_param, scale=convLayer5w_sigma_param)\n",
        "\n",
        "    # Fifth layer bias distribution priors\n",
        "    convLayer5b_mu    = torch.randn_like(net.conv5[0].bias)\n",
        "    convLayer5b_sigma = torch.randn_like(net.conv5[0].bias)\n",
        "    convLayer5b_mu_param    = pyro.param(\"convLayer5b_mu\", convLayer5b_mu)\n",
        "    convLayer5b_sigma_param = softplus(pyro.param(\"convLayer5b_sigma\", convLayer5b_sigma))\n",
        "    convLayer5_b = Normal(loc=convLayer5b_mu_param, scale=convLayer5b_sigma_param)\n",
        "\n",
        "    # First fully connected layer weight distribution priors\n",
        "    fc1w_mu = torch.randn_like(net.fc1.weight)\n",
        "    fc1w_sigma = torch.randn_like(net.fc1.weight)\n",
        "    fc1w_mu_param = pyro.param(\"fc1w_mu\", fc1w_mu)\n",
        "    fc1w_sigma_param = softplus(pyro.param(\"fc1w_sigma\", fc1w_sigma))\n",
        "    fc1Layer_w = Normal(loc=fc1w_mu_param, scale=fc1w_sigma_param).independent(1)\n",
        "\n",
        "    # First fully connected layer bias distribution priors\n",
        "    fc1b_mu = torch.randn_like(net.fc1.bias)\n",
        "    fc1b_sigma = torch.randn_like(net.fc1.bias)\n",
        "    fc1b_mu_param = pyro.param(\"fc1b_mu\", fc1b_mu)\n",
        "    fc1b_sigma_param = softplus(pyro.param(\"fc1b_sigma\", fc1b_sigma))\n",
        "    fc1Layer_b = Normal(loc=fc1b_mu_param, scale=fc1b_sigma_param)\n",
        "\n",
        "    # Second fully connected layer weight distribution priors\n",
        "    fc2w_mu = torch.randn_like(net.fc2.weight)\n",
        "    fc2w_sigma = torch.randn_like(net.fc2.weight)\n",
        "    fc2w_mu_param = pyro.param(\"fc2w_mu\", fc2w_mu)\n",
        "    fc2w_sigma_param = softplus(pyro.param(\"fc2w_sigma\", fc2w_sigma))\n",
        "    fc2Layer_w = Normal(loc=fc2w_mu_param, scale=fc2w_sigma_param).independent(1)\n",
        "\n",
        "    # Second fully connected layer bias distribution priors\n",
        "    fc2b_mu = torch.randn_like(net.fc2.bias)\n",
        "    fc2b_sigma = torch.randn_like(net.fc2.bias)\n",
        "    fc2b_mu_param = pyro.param(\"fc2b_mu\", fc2b_mu)\n",
        "    fc2b_sigma_param = softplus(pyro.param(\"fc2b_sigma\", fc2b_sigma))\n",
        "    fc2Layer_b = Normal(loc=fc2b_mu_param, scale=fc2b_sigma_param)\n",
        "\n",
        "    # Third fully connected layer weight distribution priors\n",
        "    fc3w_mu = torch.randn_like(net.fc3.weight)\n",
        "    fc3w_sigma = torch.randn_like(net.fc3.weight)\n",
        "    fc3w_mu_param = pyro.param(\"fc3w_mu\", fc3w_mu)\n",
        "    fc3w_sigma_param = softplus(pyro.param(\"fc3w_sigma\", fc3w_sigma))\n",
        "    fc3Layer_w = Normal(loc=fc3w_mu_param, scale=fc3w_sigma_param).independent(1)\n",
        "\n",
        "    # Third fully connected layer bias distribution priors\n",
        "    fc3b_mu = torch.randn_like(net.fc3.bias)\n",
        "    fc3b_sigma = torch.randn_like(net.fc3.bias)\n",
        "    fc3b_mu_param = pyro.param(\"fc3b_mu\", fc3b_mu)\n",
        "    fc3b_sigma_param = softplus(pyro.param(\"fc3b_sigma\", fc3b_sigma))\n",
        "    fc3Layer_b = Normal(loc=fc3b_mu_param, scale=fc3b_sigma_param)\n",
        "\n",
        "    priors = {'conv1[0].weight': convLayer1_w,\n",
        "              'conv1[0].bias': convLayer1_b,\n",
        "              'conv2[0].weight': convLayer2_w,\n",
        "              'conv2[0].bias': convLayer2_b,\n",
        "              'conv3[0].weight': convLayer3_w,\n",
        "              'conv3[0].bias': convLayer3_b,\n",
        "              'conv4[0].weight': convLayer4_w,\n",
        "              'conv4[0].bias': convLayer4_b,\n",
        "              'conv5[0].weight': convLayer5_w,\n",
        "              'conv5[0].bias': convLayer5_b,\n",
        "              'fc1.weight': fc1Layer_w,\n",
        "              'fc1.bias': fc1Layer_b,\n",
        "              'fc2.weight': fc2Layer_w,\n",
        "              'fc2.bias': fc2Layer_b,\n",
        "              'fc3.weight': fc3Layer_w,\n",
        "              'fc3.bias': fc3Layer_b}\n",
        "\n",
        "    lifted_module = pyro.random_module(\"module\", net, priors)\n",
        "\n",
        "    return lifted_module()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkWj6hP7m5Wb"
      },
      "source": [
        "**Load the pretrained model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjXvlXSz4KXG",
        "outputId": "ab7ab1c0-39e8-47aa-9c0f-a4dbfacd2e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-7989cab1d730>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  saved_model_dict = torch.load(os.path.join(model_path, 'model_' + model_num + \".pt\")) # model #1 in the path\n",
            "/usr/local/lib/python3.10/dist-packages/pyro/params/param_store.py:181: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(input_file)\n"
          ]
        }
      ],
      "source": [
        "saved_model_dict = torch.load(os.path.join(model_path, 'model_' + model_num + \".pt\")) # model #1 in the path\n",
        "net.load_state_dict(saved_model_dict['model'])\n",
        "guide = saved_model_dict['guide']\n",
        "pyro.get_param_store().load(os.path.join(model_path, \"model_\"+model_num+\"_params.pt\")) # model #1 in the path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcMyqXVQR7l2"
      },
      "source": [
        "**Define utils**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "04cayGzoe4zC"
      },
      "outputs": [],
      "source": [
        "def give_uncertainities(x, num_samples=1):\n",
        "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
        "    yhats = [F.log_softmax(model(x).data, dim=1) for model in sampled_models]\n",
        "    return yhats[0]\n",
        "\n",
        "def compute_evidence(image):\n",
        "    # image = image.unsqueeze(dim=0)\n",
        "    y = give_uncertainities(image)\n",
        "    return y\n",
        "\n",
        "def compute_confidence(evidence):\n",
        "    conf = torch.exp(evidence)\n",
        "    conf = conf / torch.sum(conf)\n",
        "    conf_diff = conf.sort().values[0][-1] - conf.sort().values[0][-2]\n",
        "    return conf_diff, conf\n",
        "\n",
        "\n",
        "def decide(image, threshold=10):\n",
        "    rt = 0\n",
        "    max_evidence, total_evidence = 0, 0\n",
        "    while max_evidence < threshold:\n",
        "      evidence = torch.exp(compute_evidence(image)) # becomes regular softmax\n",
        "      total_evidence = total_evidence + evidence\n",
        "      max_evidence = torch.max(total_evidence)\n",
        "      rt = rt + 1\n",
        "    choice = torch.argmax(total_evidence)\n",
        "    confidence, confidence_array = compute_confidence(total_evidence)\n",
        "    return int(choice.cpu().numpy()), rt, float(confidence.cpu().numpy()), total_evidence.cpu().numpy()\n",
        "\n",
        "def save_df(index, Choice, RT, Confidence, Threshold, Noise, Labels, image_stats, total_evidence, path):\n",
        "    # unpack image data\n",
        "    noise_max, noise_mean, image_max, image_mean, noisy_image_max, noisy_image_mean = image_stats\n",
        "\n",
        "\n",
        "    simulations = {'mnist_index': index,\n",
        "                   'choice': Choice,\n",
        "                   'rt': RT,\n",
        "                   'confidence': Confidence,\n",
        "                   'threshold': Threshold,\n",
        "                   'noise': Noise,\n",
        "                   'true label': Labels,\n",
        "                   'noise max': noise_max,\n",
        "                   'noise mean': noise_mean,\n",
        "                   'image max': image_max,\n",
        "                   'image mean': image_mean,\n",
        "                   'noisy_image max': noisy_image_max,\n",
        "                   'noisy_image mean': noisy_image_mean}\n",
        "\n",
        "    simulations.update(total_evidence)\n",
        "\n",
        "    df = pd.DataFrame(simulations)\n",
        "    df.to_csv(path)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlYwlVoYAWQn"
      },
      "source": [
        "**Simulations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3u7XgcKC4res",
        "outputId": "fb3dba75-67bb-4820-a279-f4c10c610075"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set the threshold to 3\n",
            "   Set the evidence level to 0.4\n",
            "tensor(7, device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-ae6b4f7ab558>:32: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  all_labels.append(int(label.cpu().numpy()))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2, device='cuda:0')\n",
            "tensor(1, device='cuda:0')\n",
            "tensor(8, device='cuda:0')\n",
            "tensor(1, device='cuda:0')\n",
            "tensor(1, device='cuda:0')\n",
            "tensor(8, device='cuda:0')\n",
            "tensor(8, device='cuda:0')\n",
            "tensor(7, device='cuda:0')\n",
            "tensor(8, device='cuda:0')\n",
            "tensor(8, device='cuda:0')\n",
            "tensor(8, device='cuda:0')\n",
            "tensor(5, device='cuda:0')\n",
            "tensor(8, device='cuda:0')\n",
            "tensor(1, device='cuda:0')\n",
            "tensor(5, device='cuda:0')\n",
            "tensor(9, device='cuda:0')\n",
            "tensor(7, device='cuda:0')\n",
            "tensor(3, device='cuda:0')\n",
            "tensor(8, device='cuda:0')\n",
            "tensor(8, device='cuda:0')\n",
            "tensor(8, device='cuda:0')\n",
            "tensor(1, device='cuda:0')\n",
            "tensor(8, device='cuda:0')\n",
            "tensor(8, device='cuda:0')\n",
            "tensor(0, device='cuda:0')\n",
            "tensor(7, device='cuda:0')\n",
            "tensor(9, device='cuda:0')\n",
            "tensor(1, device='cuda:0')\n",
            "tensor(8, device='cuda:0')\n",
            "tensor(3, device='cuda:0')\n",
            "tensor(8, device='cuda:0')\n",
            "tensor(8, device='cuda:0')\n",
            "tensor(8, device='cuda:0')\n",
            "tensor(7, device='cuda:0')\n",
            "tensor(2, device='cuda:0')\n",
            "tensor(7, device='cuda:0')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-ae6b4f7ab558>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mnoisy_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mev\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnoise_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Get data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mchoice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_evidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mall_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mall_choice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-5fd2ba674892>\u001b[0m in \u001b[0;36mdecide\u001b[0;34m(image, threshold)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmax_evidence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_evidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mmax_evidence\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m       \u001b[0mevidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_evidence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# becomes regular softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m       \u001b[0mtotal_evidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_evidence\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mevidence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0mmax_evidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_evidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-5fd2ba674892>\u001b[0m in \u001b[0;36mcompute_evidence\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_evidence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# image = image.unsqueeze(dim=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgive_uncertainities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-5fd2ba674892>\u001b[0m in \u001b[0;36mgive_uncertainities\u001b[0;34m(x, num_samples)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgive_uncertainities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msampled_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0myhats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msampled_models\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0myhats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-5fd2ba674892>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgive_uncertainities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msampled_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0myhats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msampled_models\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0myhats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-32dbd9dd5348>\u001b[0m in \u001b[0;36mguide\u001b[0;34m(x_data, y_data)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mfc1w_mu_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fc1w_mu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc1w_mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mfc1w_sigma_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fc1w_sigma\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc1w_sigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mfc1Layer_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfc1w_mu_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfc1w_sigma_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindependent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# First fully connected layer bias distribution priors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributions/normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                     raise ValueError(\n\u001b[1;32m     72\u001b[0m                         \u001b[0;34mf\"Expected parameter {param} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "threshold_levels = [3]\n",
        "# Initialize lists\n",
        "all_index, all_choice, all_rt, all_confidence, all_threshold, all_labels, all_ev = [], [], [], [], [], [], []\n",
        "# total evidence dictionary initialization\n",
        "evidence_dict = {(\"ev\"+str(i)):[] for i in range(10)} # contains empty lists per digit class\n",
        "\n",
        "# image statistics lists\n",
        "noise_max, noise_mean, image_max, image_mean, noisy_image_max, noisy_image_mean = [], [], [], [], [], []\n",
        "\n",
        "# Iterate across threshold levels.\n",
        "for a in threshold_levels:\n",
        "  print(\"Set the threshold to {}\".format(str(a)))\n",
        "  # Iterate across levels of evidence strength (ef=evidence factor). 0 = pure noise, 1 = pure evidence\n",
        "  for ev in np.arange(0.4, 0.6, 0.1).round(1):\n",
        "      # Set save_path\n",
        "      save_path = os.path.join(save_dir, \"model_\" + model_num + '_evidence_array_sims_' + str(a) + \"_\" + str(ev).replace(\".\", \"_\") + \".csv\")\n",
        "      print('   Set the evidence level to {}'.format(str(ev)))\n",
        "      for i, (image, label) in enumerate(test_loader):\n",
        "        # Compute final image as a weighted blend between pure noise and the original image\n",
        "        # Compute noise array\n",
        "        noise_array = torch.rand(image.shape) * image.max() # multiply with the max value to make the noise as strong as the original image.\n",
        "        # Compute final image\n",
        "        noisy_image = ev*image + (1-ev)*noise_array\n",
        "        # Get data\n",
        "        choice, rt, confidence, total_evidence = decide(noisy_image.to(device), threshold=a)\n",
        "        all_index.append(i)\n",
        "        all_choice.append(choice)\n",
        "        all_rt.append(rt)\n",
        "        all_confidence.append(confidence)\n",
        "        all_threshold.append(a)\n",
        "        all_ev.append(ev)\n",
        "        all_labels.append(int(label.cpu().numpy()))\n",
        "\n",
        "        # Append evidence per digit class to corresponding dictionary key\n",
        "        for i, evid in enumerate(evidence_dict):\n",
        "          evidence_dict[evid].append(total_evidence[0][i])\n",
        "\n",
        "        # image and noise data\n",
        "        noise_max.append(noise_array.max().cpu().numpy())\n",
        "        noise_mean.append(noise_array.mean().cpu().numpy())\n",
        "        image_max.append(image[image>0].max().cpu().numpy()) # Just take the positive non-zero values (the actual pixels belonging to the evidence)\n",
        "        image_mean.append(image[image>0].mean().cpu().numpy())\n",
        "        noisy_image_max.append(noisy_image.max().cpu().numpy())\n",
        "        noisy_image_mean.append(noisy_image.mean().cpu().numpy())\n",
        "\n",
        "        # Print progress after each 1000 images.\n",
        "        if (i%1000)==0:\n",
        "          print('           {}'.format(str(i)))\n",
        "\n",
        "      # store image stats in single list\n",
        "      all_image_stats = [noise_max, noise_mean, image_max, image_mean, noisy_image_max, noisy_image_mean]\n",
        "      # Save to file.\n",
        "      save_df(all_index, all_choice, all_rt, all_confidence, all_threshold, all_ev, all_labels, all_image_stats, evidence_dict, save_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ev"
      ],
      "metadata": {
        "id": "WfODOEi696Hh",
        "outputId": "5ef3ab89-defa-4270-98e2-c9bf6240d9b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ev9'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E9lYY5qphf9"
      },
      "source": [
        "**Example Images with Noise**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "FnbyZldWpgnI",
        "outputId": "7e22d41f-ac1a-4112-9e91-b45de98873f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'results/evidence_level_sims_5_0_1.csv'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = 5\n",
        "ev = 0.1\n",
        "save_path = 'results/evidence_level_sims_' + str(a) + \"_\" + str(ev).replace(\".\", \"_\") + \".csv\"\n",
        "\n",
        "save_path"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlSS96RwlFrT"
   },
   "source": [
    "**How to run this script?**\n",
    "\n",
    "\n",
    "1.   Navigate to \"**Load the pretrained model**\" section and set the path\n",
    "where you located the Bayesian models.\n",
    "2.   Navigate to \"**Simulations**\" section and provide a location where you want to save the model output.\n",
    "3. You are all set! Now you can run the script on Google Colab.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4JAgS4o3mg5",
    "outputId": "a3a366cc-40f2-469c-ad66-63b215eca3d0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Switch\n",
    "local = True\n",
    "\n",
    "if local:\n",
    "    mnist_data_path = os.path.join(\"..\", \"..\", \"data\", \"mnist-data\")\n",
    "\n",
    "    # Set paths\n",
    "    model_num = \"01\"\n",
    "    model_path = os.path.join(\"..\", \"..\", \"data\", \"Bayesian_models\")\n",
    "    # Path for saving results\n",
    "    save_dir = os.path.join(\"..\", \"..\", \"results\")\n",
    "\n",
    "else: # We're on colab\n",
    "    from google.colab import drive\n",
    "    # Install pyro package\n",
    "    !pip3 install pyro-ppl==0.2.1\n",
    "    # Set different mnist path\n",
    "    mnist_data_path = os.path.join(\"mnist-data\")\n",
    "\n",
    "    # Mount google drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Set paths\n",
    "    model_num = \"01\"\n",
    "    model_path = os.path.join(\"drive\", \"MyDrive\", \"RTNet\", \"models\")\n",
    "    # Path for saving results\n",
    "    save_dir = os.path.join(\"drive\", \"MyDrive\", \"RTNet\", \"results\")\n",
    "\n",
    "# Check whether folders exist\n",
    "assert os.path.exists(model_path)\n",
    "assert os.path.exists(save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "N6jMxsF_xVp4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import Normal, Categorical\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "\n",
    "#from google.colab import files\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7rmEFaPL30Sh",
    "outputId": "004fe927-44fd-4cc7-a3cc-e98f655e1033"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0DJT0xeAqXL"
   },
   "source": [
    "**Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tTRmzMB4yMxj",
    "outputId": "3d659062-7d05-43e7-83ee-49f1e1c5c17e"
   },
   "outputs": [],
   "source": [
    "AlexTransform = transforms.Compose([\n",
    "    transforms.Resize((227, 227)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(mnist_data_path, train=True, download=True, transform=AlexTransform),\n",
    "        batch_size=500, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(mnist_data_path, train=False, download=True, transform=AlexTransform),\n",
    "        batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Qnn6wDRi8zh0"
   },
   "outputs": [],
   "source": [
    "# # View images\n",
    "# complete_im = []\n",
    "# images_high_conf = [5283, 5726, 1099, 792, 8199, 6817, 3376, 9530, 1150, 3497]\n",
    "# images_low_conf = [7736, 8151, 7701, 8537, 9359, 6520, 4316, 1452, 3699, 9261]\n",
    "# for idx in images_high_conf:\n",
    "#     im, _ = test_loader.dataset[idx]\n",
    "#     complete_im.append(im)\n",
    "\n",
    "# images = torchvision.utils.make_grid(complete_im)\n",
    "\n",
    "# img = torchvision.transforms.ToPILImage()(images)\n",
    "# img.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyqaSFcs3Udj"
   },
   "source": [
    "**Define the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4QqsElxSF2F"
   },
   "source": [
    "AlexNet structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XNBayOSDyyn9"
   },
   "outputs": [],
   "source": [
    "# AlexNet\n",
    "class alexnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=96, kernel_size=11, stride=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, 3, 1, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 384, 3, 1, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.conv5(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        out = F.relu(self.fc1(out))  # 256*6*6 -> 4096\n",
    "        out = F.dropout(out, 0.5)\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = F.dropout(out, 0.5)\n",
    "        out = self.fc3(out)\n",
    "        # out = F.log_softmax(out, dim=1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "l-94O5iW367h"
   },
   "outputs": [],
   "source": [
    "# Define Hyper parameters\n",
    "img_size = 28 * 28\n",
    "hidden_layer_size = 1024\n",
    "num_classes = 10\n",
    "net = alexnet().to(device)\n",
    "# softmax\n",
    "log_softmax = nn.LogSoftmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COLZgZNISK4g"
   },
   "source": [
    "Model function for pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "liRRzYaD4Ejg"
   },
   "outputs": [],
   "source": [
    "def model(x_data, y_data):\n",
    "\n",
    "    convLayer1_w = Normal(loc=torch.ones_like(net.conv1[0].weight), scale=torch.ones_like(net.conv1[0].weight))\n",
    "    convLayer1_b = Normal(loc=torch.ones_like(net.conv1[0].bias), scale=torch.ones_like(net.conv1[0].bias))\n",
    "\n",
    "    convLayer2_w = Normal(loc=torch.ones_like(net.conv2[0].weight), scale=torch.ones_like(net.conv2[0].weight))\n",
    "    convLayer2_b = Normal(loc=torch.ones_like(net.conv2[0].bias), scale=torch.ones_like(net.conv2[0].bias))\n",
    "\n",
    "    convLayer3_w = Normal(loc=torch.ones_like(net.conv3[0].weight), scale=torch.ones_like(net.conv3[0].weight))\n",
    "    convLayer3_b = Normal(loc=torch.ones_like(net.conv3[0].bias), scale=torch.ones_like(net.conv3[0].bias))\n",
    "\n",
    "    convLayer4_w = Normal(loc=torch.ones_like(net.conv4[0].weight), scale=torch.ones_like(net.conv4[0].weight))\n",
    "    convLayer4_b = Normal(loc=torch.ones_like(net.conv4[0].bias), scale=torch.ones_like(net.conv4[0].bias))\n",
    "\n",
    "    convLayer5_w = Normal(loc=torch.ones_like(net.conv5[0].weight), scale=torch.ones_like(net.conv5[0].weight))\n",
    "    convLayer5_b = Normal(loc=torch.ones_like(net.conv5[0].bias), scale=torch.ones_like(net.conv5[0].bias))\n",
    "\n",
    "    fc1Layer_w = Normal(loc=torch.ones_like(net.fc1.weight), scale=torch.ones_like(net.fc1.weight))\n",
    "    fc1Layer_b = Normal(loc=torch.ones_like(net.fc1.bias), scale=torch.ones_like(net.fc1.bias))\n",
    "\n",
    "    fc2Layer_w = Normal(loc=torch.ones_like(net.fc2.weight), scale=torch.ones_like(net.fc2.weight))\n",
    "    fc2Layer_b = Normal(loc=torch.ones_like(net.fc2.bias), scale=torch.ones_like(net.fc2.bias))\n",
    "\n",
    "    fc3Layer_w = Normal(loc=torch.ones_like(net.fc3.weight), scale=torch.ones_like(net.fc3.weight))\n",
    "    fc3Layer_b = Normal(loc=torch.ones_like(net.fc3.bias), scale=torch.ones_like(net.fc3.bias))\n",
    "\n",
    "    priors = {'conv1[0].weight': convLayer1_w,\n",
    "              'conv1[0].bias': convLayer1_b,\n",
    "              'conv2[0].weight': convLayer2_w,\n",
    "              'conv2[0].bias': convLayer2_b,\n",
    "              'conv3[0].weight': convLayer3_w,\n",
    "              'conv3[0].bias': convLayer3_b,\n",
    "              'conv4[0].weight': convLayer4_w,\n",
    "              'conv4[0].bias': convLayer4_b,\n",
    "              'conv5[0].weight': convLayer5_w,\n",
    "              'conv5[0].bias': convLayer5_b,\n",
    "              'fc1.weight': fc1Layer_w,\n",
    "              'fc1.bias': fc1Layer_b,\n",
    "              'fc2.weight': fc2Layer_w,\n",
    "              'fc2.bias': fc2Layer_b,\n",
    "              'fc3.weight': fc3Layer_w,\n",
    "              'fc3.bias': fc3Layer_b}\n",
    "\n",
    "    # lift module parameters to random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module(\"module\", net, priors)\n",
    "    # sample a regressor (which also samples w and b)\n",
    "    lifted_reg_model = lifted_module()\n",
    "\n",
    "    lhat = log_softmax(lifted_reg_model(x_data))\n",
    "\n",
    "    pyro.sample(\"obs\", Categorical(logits=lhat), obs=y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hX4OnUndST19"
   },
   "source": [
    "Guide function for pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "So18-_z_4Hv3"
   },
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "\n",
    "def guide(x_data, y_data):\n",
    "\n",
    "    # First layer weight distribution priors\n",
    "    convLayer1w_mu    = torch.randn_like(net.conv1[0].weight)\n",
    "    convLayer1w_sigma = torch.randn_like(net.conv1[0].weight)\n",
    "    convLayer1w_mu_param    = pyro.param(\"convLayer1w_mu\", convLayer1w_mu)\n",
    "    convLayer1w_sigma_param = softplus(pyro.param(\"convLayer1w_sigma\", convLayer1w_sigma))\n",
    "    convLayer1_w = Normal(loc=convLayer1w_mu_param, scale=convLayer1w_sigma_param)\n",
    "\n",
    "    # First layer bias distribution priors\n",
    "    convLayer1b_mu    = torch.randn_like(net.conv1[0].bias)\n",
    "    convLayer1b_sigma = torch.randn_like(net.conv1[0].bias)\n",
    "    convLayer1b_mu_param    = pyro.param(\"convLayer1b_mu\", convLayer1b_mu)\n",
    "    convLayer1b_sigma_param = softplus(pyro.param(\"convLayer1b_sigma\", convLayer1b_sigma))\n",
    "    convLayer1_b = Normal(loc=convLayer1b_mu_param, scale=convLayer1b_sigma_param)\n",
    "\n",
    "    # Second layer weight distribution priors\n",
    "    convLayer2w_mu    = torch.randn_like(net.conv2[0].weight)\n",
    "    convLayer2w_sigma = torch.randn_like(net.conv2[0].weight)\n",
    "    convLayer2w_mu_param    = pyro.param(\"convLayer2w_mu\", convLayer2w_mu)\n",
    "    convLayer2w_sigma_param = softplus(pyro.param(\"convLayer2w_sigma\", convLayer2w_sigma))\n",
    "    convLayer2_w = Normal(loc=convLayer2w_mu_param, scale=convLayer2w_sigma_param)\n",
    "\n",
    "    # Second layer bias distribution priors\n",
    "    convLayer2b_mu    = torch.randn_like(net.conv2[0].bias)\n",
    "    convLayer2b_sigma = torch.randn_like(net.conv2[0].bias)\n",
    "    convLayer2b_mu_param    = pyro.param(\"convLayer2b_mu\", convLayer2b_mu)\n",
    "    convLayer2b_sigma_param = softplus(pyro.param(\"convLayer2b_sigma\", convLayer2b_sigma))\n",
    "    convLayer2_b = Normal(loc=convLayer2b_mu_param, scale=convLayer2b_sigma_param)\n",
    "\n",
    "    # Third layer weight distribution priors\n",
    "    convLayer3w_mu    = torch.randn_like(net.conv3[0].weight)\n",
    "    convLayer3w_sigma = torch.randn_like(net.conv3[0].weight)\n",
    "    convLayer3w_mu_param    = pyro.param(\"convLayer3w_mu\", convLayer3w_mu)\n",
    "    convLayer3w_sigma_param = softplus(pyro.param(\"convLayer3w_sigma\", convLayer3w_sigma))\n",
    "    convLayer3_w = Normal(loc=convLayer3w_mu_param, scale=convLayer3w_sigma_param)\n",
    "\n",
    "    # Third layer bias distribution priors\n",
    "    convLayer3b_mu    = torch.randn_like(net.conv3[0].bias)\n",
    "    convLayer3b_sigma = torch.randn_like(net.conv3[0].bias)\n",
    "    convLayer3b_mu_param    = pyro.param(\"convLayer3b_mu\", convLayer3b_mu)\n",
    "    convLayer3b_sigma_param = softplus(pyro.param(\"convLayer3b_sigma\", convLayer3b_sigma))\n",
    "    convLayer3_b = Normal(loc=convLayer3b_mu_param, scale=convLayer3b_sigma_param)\n",
    "\n",
    "    # Fourth layer weight distribution priors\n",
    "    convLayer4w_mu    = torch.randn_like(net.conv4[0].weight)\n",
    "    convLayer4w_sigma = torch.randn_like(net.conv4[0].weight)\n",
    "    convLayer4w_mu_param    = pyro.param(\"convLayer4w_mu\", convLayer4w_mu)\n",
    "    convLayer4w_sigma_param = softplus(pyro.param(\"convLayer4w_sigma\", convLayer4w_sigma))\n",
    "    convLayer4_w = Normal(loc=convLayer4w_mu_param, scale=convLayer4w_sigma_param)\n",
    "\n",
    "    # Fourth layer bias distribution priors\n",
    "    convLayer4b_mu    = torch.randn_like(net.conv4[0].bias)\n",
    "    convLayer4b_sigma = torch.randn_like(net.conv4[0].bias)\n",
    "    convLayer4b_mu_param    = pyro.param(\"convLayer4b_mu\", convLayer4b_mu)\n",
    "    convLayer4b_sigma_param = softplus(pyro.param(\"convLayer4b_sigma\", convLayer4b_sigma))\n",
    "    convLayer4_b = Normal(loc=convLayer4b_mu_param, scale=convLayer4b_sigma_param)\n",
    "\n",
    "    # Fifth layer weight distribution priors\n",
    "    convLayer5w_mu    = torch.randn_like(net.conv5[0].weight)\n",
    "    convLayer5w_sigma = torch.randn_like(net.conv5[0].weight)\n",
    "    convLayer5w_mu_param    = pyro.param(\"convLayer5w_mu\", convLayer5w_mu)\n",
    "    convLayer5w_sigma_param = softplus(pyro.param(\"convLayer5w_sigma\", convLayer5w_sigma))\n",
    "    convLayer5_w = Normal(loc=convLayer5w_mu_param, scale=convLayer5w_sigma_param)\n",
    "\n",
    "    # Fifth layer bias distribution priors\n",
    "    convLayer5b_mu    = torch.randn_like(net.conv5[0].bias)\n",
    "    convLayer5b_sigma = torch.randn_like(net.conv5[0].bias)\n",
    "    convLayer5b_mu_param    = pyro.param(\"convLayer5b_mu\", convLayer5b_mu)\n",
    "    convLayer5b_sigma_param = softplus(pyro.param(\"convLayer5b_sigma\", convLayer5b_sigma))\n",
    "    convLayer5_b = Normal(loc=convLayer5b_mu_param, scale=convLayer5b_sigma_param)\n",
    "\n",
    "    # First fully connected layer weight distribution priors\n",
    "    fc1w_mu = torch.randn_like(net.fc1.weight)\n",
    "    fc1w_sigma = torch.randn_like(net.fc1.weight)\n",
    "    fc1w_mu_param = pyro.param(\"fc1w_mu\", fc1w_mu)\n",
    "    fc1w_sigma_param = softplus(pyro.param(\"fc1w_sigma\", fc1w_sigma))\n",
    "    fc1Layer_w = Normal(loc=fc1w_mu_param, scale=fc1w_sigma_param).independent(1)\n",
    "\n",
    "    # First fully connected layer bias distribution priors\n",
    "    fc1b_mu = torch.randn_like(net.fc1.bias)\n",
    "    fc1b_sigma = torch.randn_like(net.fc1.bias)\n",
    "    fc1b_mu_param = pyro.param(\"fc1b_mu\", fc1b_mu)\n",
    "    fc1b_sigma_param = softplus(pyro.param(\"fc1b_sigma\", fc1b_sigma))\n",
    "    fc1Layer_b = Normal(loc=fc1b_mu_param, scale=fc1b_sigma_param)\n",
    "\n",
    "    # Second fully connected layer weight distribution priors\n",
    "    fc2w_mu = torch.randn_like(net.fc2.weight)\n",
    "    fc2w_sigma = torch.randn_like(net.fc2.weight)\n",
    "    fc2w_mu_param = pyro.param(\"fc2w_mu\", fc2w_mu)\n",
    "    fc2w_sigma_param = softplus(pyro.param(\"fc2w_sigma\", fc2w_sigma))\n",
    "    fc2Layer_w = Normal(loc=fc2w_mu_param, scale=fc2w_sigma_param).independent(1)\n",
    "\n",
    "    # Second fully connected layer bias distribution priors\n",
    "    fc2b_mu = torch.randn_like(net.fc2.bias)\n",
    "    fc2b_sigma = torch.randn_like(net.fc2.bias)\n",
    "    fc2b_mu_param = pyro.param(\"fc2b_mu\", fc2b_mu)\n",
    "    fc2b_sigma_param = softplus(pyro.param(\"fc2b_sigma\", fc2b_sigma))\n",
    "    fc2Layer_b = Normal(loc=fc2b_mu_param, scale=fc2b_sigma_param)\n",
    "\n",
    "    # Third fully connected layer weight distribution priors\n",
    "    fc3w_mu = torch.randn_like(net.fc3.weight)\n",
    "    fc3w_sigma = torch.randn_like(net.fc3.weight)\n",
    "    fc3w_mu_param = pyro.param(\"fc3w_mu\", fc3w_mu)\n",
    "    fc3w_sigma_param = softplus(pyro.param(\"fc3w_sigma\", fc3w_sigma))\n",
    "    fc3Layer_w = Normal(loc=fc3w_mu_param, scale=fc3w_sigma_param).independent(1)\n",
    "\n",
    "    # Third fully connected layer bias distribution priors\n",
    "    fc3b_mu = torch.randn_like(net.fc3.bias)\n",
    "    fc3b_sigma = torch.randn_like(net.fc3.bias)\n",
    "    fc3b_mu_param = pyro.param(\"fc3b_mu\", fc3b_mu)\n",
    "    fc3b_sigma_param = softplus(pyro.param(\"fc3b_sigma\", fc3b_sigma))\n",
    "    fc3Layer_b = Normal(loc=fc3b_mu_param, scale=fc3b_sigma_param)\n",
    "\n",
    "    priors = {'conv1[0].weight': convLayer1_w,\n",
    "              'conv1[0].bias': convLayer1_b,\n",
    "              'conv2[0].weight': convLayer2_w,\n",
    "              'conv2[0].bias': convLayer2_b,\n",
    "              'conv3[0].weight': convLayer3_w,\n",
    "              'conv3[0].bias': convLayer3_b,\n",
    "              'conv4[0].weight': convLayer4_w,\n",
    "              'conv4[0].bias': convLayer4_b,\n",
    "              'conv5[0].weight': convLayer5_w,\n",
    "              'conv5[0].bias': convLayer5_b,\n",
    "              'fc1.weight': fc1Layer_w,\n",
    "              'fc1.bias': fc1Layer_b,\n",
    "              'fc2.weight': fc2Layer_w,\n",
    "              'fc2.bias': fc2Layer_b,\n",
    "              'fc3.weight': fc3Layer_w,\n",
    "              'fc3.bias': fc3Layer_b}\n",
    "\n",
    "    lifted_module = pyro.random_module(\"module\", net, priors)\n",
    "\n",
    "    return lifted_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkWj6hP7m5Wb"
   },
   "source": [
    "**Load the pretrained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SjXvlXSz4KXG",
    "outputId": "ab7ab1c0-39e8-47aa-9c0f-a4dbfacd2e10"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomva\\AppData\\Local\\Temp\\ipykernel_21856\\716814525.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  saved_model_dict = torch.load(os.path.join(model_path, 'model_' + model_num + \".pt\")) # model #1 in the path\n",
      "c:\\Users\\tomva\\OneDrive\\KU Leuven\\Master Theory and Research\\Internship\\RTNet\\venv\\lib\\site-packages\\pyro\\params\\param_store.py:181: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(input_file)\n"
     ]
    }
   ],
   "source": [
    "saved_model_dict = torch.load(os.path.join(model_path, 'model_' + model_num + \".pt\")) # model #1 in the path\n",
    "net.load_state_dict(saved_model_dict['model'])\n",
    "guide = saved_model_dict['guide']\n",
    "pyro.get_param_store().load(os.path.join(model_path, \"model_\"+model_num+\"_params.pt\")) # model #1 in the path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcMyqXVQR7l2"
   },
   "source": [
    "**Define utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04cayGzoe4zC"
   },
   "outputs": [],
   "source": [
    "def give_uncertainities(x, num_samples=1):\n",
    "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
    "    yhats = [model(x).data for model in sampled_models]\n",
    "    return yhats[0]\n",
    "\n",
    "def compute_evidence(image):\n",
    "    # image = image.unsqueeze(dim=0)\n",
    "    y = give_uncertainities(image)\n",
    "    return y\n",
    "\n",
    "def compute_confidence(evidence):\n",
    "    conf = evidence\n",
    "    conf_diff = conf.sort().values[0][-1] - conf.sort().values[0][-2]\n",
    "    return conf_diff\n",
    "\n",
    "\n",
    "def decide(image, threshold=10):\n",
    "    rt = 0\n",
    "    max_evidence, total_evidence, total_evidence_for_conf = 0, 0, 0\n",
    "    while max_evidence < threshold:\n",
    "        evidence_for_conf = compute_evidence(image) # raw activations\n",
    "        total_evidence_for_conf = total_evidence_for_conf + evidence_for_conf # We use this for confidence computation\n",
    "        evidence = torch.exp(F.log_softmax(evidence_for_conf)) \n",
    "        total_evidence = total_evidence + evidence\n",
    "        max_evidence = torch.max(total_evidence)\n",
    "        rt = rt + 1\n",
    "    choice = torch.argmax(total_evidence)\n",
    "    confidence = compute_confidence(total_evidence_for_conf)\n",
    "    return int(choice.cpu().numpy()), rt, float(confidence.cpu().numpy()), total_evidence.cpu().numpy()\n",
    "\n",
    "def save_df(index, Choice, RT, Confidence, Threshold, Noise, Labels, image_stats, total_evidence, path):\n",
    "    # unpack image data\n",
    "    noise_max, noise_mean, image_max, image_mean, noisy_image_max, noisy_image_mean = image_stats\n",
    "\n",
    "\n",
    "    simulations = {'mnist_index': index,\n",
    "                   'choice': Choice,\n",
    "                   'rt': RT,\n",
    "                   'confidence': Confidence,\n",
    "                   'threshold': Threshold,\n",
    "                   'noise': Noise,\n",
    "                   'true label': Labels,\n",
    "                   'noise max': noise_max,\n",
    "                   'noise mean': noise_mean,\n",
    "                   'image max': image_max,\n",
    "                   'image mean': image_mean,\n",
    "                   'noisy_image max': noisy_image_max,\n",
    "                   'noisy_image mean': noisy_image_mean}\n",
    "\n",
    "    simulations.update(total_evidence)\n",
    "\n",
    "    df = pd.DataFrame(simulations)\n",
    "    df.to_csv(path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlYwlVoYAWQn"
   },
   "source": [
    "**Simulations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3u7XgcKC4res",
    "outputId": "fb3dba75-67bb-4820-a279-f4c10c610075"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set the threshold to 3\n",
      "   Set the evidence level to 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomva\\AppData\\Local\\Temp\\ipykernel_21856\\3413726523.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  evidence = torch.exp(F.log_softmax(evidence_for_conf))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total evidence: tensor([[  3302.5295,   5150.7363,    683.2744,   3237.5439,  -4103.4600,\n",
      "          -4584.6475, -13697.1250,  20096.3359,   5372.1055,  -6294.3613]],\n",
      "       device='cuda:0')\n",
      "Confidence: 14724.23046875\n",
      "Max evidence: 3.0\n",
      "Total evidence: tensor([[  5630.0562,   4494.0698,  11372.5039,  -2865.7354,  -8617.8057,\n",
      "         -10357.9336,  10427.6943,  -9325.8760,   2254.1880,  -9052.3076]],\n",
      "       device='cuda:0')\n",
      "Confidence: 944.8095703125\n",
      "Max evidence: 3.0\n",
      "Total evidence: tensor([[-11180.3379,  22997.0723,  -4791.8901,   -253.4923,   1138.5066,\n",
      "          -2820.6899,   -115.3616,   5782.6479,   6228.8828,  -6068.5327]],\n",
      "       device='cuda:0')\n",
      "Confidence: 16768.189453125\n",
      "Max evidence: 3.0\n",
      "Total evidence: tensor([[  3345.3210,  13023.8896,  -8580.4482,  -6055.6025, -10856.9629,\n",
      "           9048.5869,  -4383.2051,  -7296.2290,  12425.6768, -11058.0410]],\n",
      "       device='cuda:0')\n",
      "Confidence: 598.212890625\n",
      "Max evidence: 3.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m noisy_image \u001b[38;5;241m=\u001b[39m ev\u001b[38;5;241m*\u001b[39mimage \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mev)\u001b[38;5;241m*\u001b[39mnoise_array\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Get data\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m choice, rt, confidence, total_evidence \u001b[38;5;241m=\u001b[39m \u001b[43mdecide\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m all_index\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[0;32m     27\u001b[0m all_choice\u001b[38;5;241m.\u001b[39mappend(choice)\n",
      "Cell \u001b[1;32mIn[20], line 20\u001b[0m, in \u001b[0;36mdecide\u001b[1;34m(image, threshold)\u001b[0m\n\u001b[0;32m     18\u001b[0m rt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     19\u001b[0m max_evidence, total_evidence, total_evidence_for_conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43mmax_evidence\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m:\n\u001b[0;32m     21\u001b[0m     evidence_for_conf \u001b[38;5;241m=\u001b[39m compute_evidence(image) \u001b[38;5;66;03m# raw activations\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     total_evidence_for_conf \u001b[38;5;241m=\u001b[39m total_evidence_for_conf \u001b[38;5;241m+\u001b[39m evidence_for_conf \u001b[38;5;66;03m# We use this for confidence computation\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "threshold_levels = [3]\n",
    "# Initialize lists\n",
    "all_index, all_choice, all_rt, all_confidence, all_threshold, all_labels, all_ev = [], [], [], [], [], [], []\n",
    "# total evidence dictionary initialization\n",
    "evidence_dict = {(\"ev\"+str(i)):[] for i in range(10)} # contains empty lists per digit class\n",
    "\n",
    "# image statistics lists\n",
    "noise_max, noise_mean, image_max, image_mean, noisy_image_max, noisy_image_mean = [], [], [], [], [], []\n",
    "\n",
    "# Iterate across threshold levels.\n",
    "for a in threshold_levels:\n",
    "  print(\"Set the threshold to {}\".format(str(a)))\n",
    "  # Iterate across levels of evidence strength (ef=evidence factor). 0 = pure noise, 1 = pure evidence\n",
    "  for ev in np.arange(0.4, 0.6, 0.1).round(1):\n",
    "      # Set save_path\n",
    "      save_path = os.path.join(save_dir, \"model_\" + model_num + '_evidence_array_sims_' + str(a) + \"_\" + str(ev).replace(\".\", \"_\") + \".csv\")\n",
    "      print('   Set the evidence level to {}'.format(str(ev)))\n",
    "      for i, (image, label) in enumerate(test_loader):\n",
    "        # Compute final image as a weighted blend between pure noise and the original image\n",
    "        # Compute noise array\n",
    "        noise_array = torch.rand(image.shape) * image.max() # multiply with the max value to make the noise as strong as the original image.\n",
    "        # Compute final image\n",
    "        noisy_image = ev*image + (1-ev)*noise_array\n",
    "        # Get data\n",
    "        choice, rt, confidence, total_evidence = decide(noisy_image.to(device), threshold=a)\n",
    "        all_index.append(i)\n",
    "        all_choice.append(choice)\n",
    "        all_rt.append(rt)\n",
    "        all_confidence.append(confidence)\n",
    "        all_threshold.append(a)\n",
    "        all_ev.append(ev)\n",
    "        all_labels.append(int(label.cpu().numpy()))\n",
    "\n",
    "        # Append evidence per digit class to corresponding dictionary key\n",
    "        for i, evid in enumerate(evidence_dict):\n",
    "          evidence_dict[evid].append(total_evidence[0][i])\n",
    "\n",
    "        # image and noise data\n",
    "        noise_max.append(noise_array.max().cpu().numpy())\n",
    "        noise_mean.append(noise_array.mean().cpu().numpy())\n",
    "        image_max.append(image[image>0].max().cpu().numpy()) # Just take the positive non-zero values (the actual pixels belonging to the evidence)\n",
    "        image_mean.append(image[image>0].mean().cpu().numpy())\n",
    "        noisy_image_max.append(noisy_image.max().cpu().numpy())\n",
    "        noisy_image_mean.append(noisy_image.mean().cpu().numpy())\n",
    "\n",
    "        # Print progress after each 1000 images.\n",
    "        if (i%1000)==0:\n",
    "          print('           {}'.format(str(i)))\n",
    "\n",
    "      # store image stats in single list\n",
    "      all_image_stats = [noise_max, noise_mean, image_max, image_mean, noisy_image_max, noisy_image_mean]\n",
    "      # Save to file.\n",
    "      save_df(all_index, all_choice, all_rt, all_confidence, all_threshold, all_ev, all_labels, all_image_stats, evidence_dict, save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "WfODOEi696Hh",
    "outputId": "5ef3ab89-defa-4270-98e2-c9bf6240d9b2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'ev9'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6E9lYY5qphf9"
   },
   "source": [
    "**Example Images with Noise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "FnbyZldWpgnI",
    "outputId": "7e22d41f-ac1a-4112-9e91-b45de98873f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results/evidence_level_sims_5_0_1.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 5\n",
    "ev = 0.1\n",
    "save_path = 'results/evidence_level_sims_' + str(a) + \"_\" + str(ev).replace(\".\", \"_\") + \".csv\"\n",
    "\n",
    "save_path"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

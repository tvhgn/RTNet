{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlSS96RwlFrT"
   },
   "source": [
    "### **Train a Bayesian model from scratch for MNIST using AlexNet architecture**\n",
    "\n",
    "The following notebook can be run on google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4JAgS4o3mg5",
    "outputId": "d21b31ee-f370-4fbf-931c-6088a5b7ff69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyro-ppl==0.2.1 in c:\\users\\tomva\\onedrive\\ku leuven\\master theory and research\\internship\\rtnet\\venv\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: contextlib2 in c:\\users\\tomva\\onedrive\\ku leuven\\master theory and research\\internship\\rtnet\\venv\\lib\\site-packages (from pyro-ppl==0.2.1) (21.6.0)\n",
      "Requirement already satisfied: graphviz>=0.8 in c:\\users\\tomva\\onedrive\\ku leuven\\master theory and research\\internship\\rtnet\\venv\\lib\\site-packages (from pyro-ppl==0.2.1) (0.20.3)\n",
      "Requirement already satisfied: networkx>=2.0.0 in c:\\users\\tomva\\onedrive\\ku leuven\\master theory and research\\internship\\rtnet\\venv\\lib\\site-packages (from pyro-ppl==0.2.1) (3.1)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\tomva\\onedrive\\ku leuven\\master theory and research\\internship\\rtnet\\venv\\lib\\site-packages (from pyro-ppl==0.2.1) (1.24.4)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\tomva\\onedrive\\ku leuven\\master theory and research\\internship\\rtnet\\venv\\lib\\site-packages (from pyro-ppl==0.2.1) (1.16.0)\n",
      "Requirement already satisfied: torch>=0.4.0 in c:\\users\\tomva\\onedrive\\ku leuven\\master theory and research\\internship\\rtnet\\venv\\lib\\site-packages (from pyro-ppl==0.2.1) (2.4.0+cu124)\n",
      "Requirement already satisfied: filelock in c:\\users\\tomva\\onedrive\\ku leuven\\master theory and research\\internship\\rtnet\\venv\\lib\\site-packages (from torch>=0.4.0->pyro-ppl==0.2.1) (3.15.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\tomva\\onedrive\\ku leuven\\master theory and research\\internship\\rtnet\\venv\\lib\\site-packages (from torch>=0.4.0->pyro-ppl==0.2.1) (1.13.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tomva\\onedrive\\ku leuven\\master theory and research\\internship\\rtnet\\venv\\lib\\site-packages (from torch>=0.4.0->pyro-ppl==0.2.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tomva\\onedrive\\ku leuven\\master theory and research\\internship\\rtnet\\venv\\lib\\site-packages (from torch>=0.4.0->pyro-ppl==0.2.1) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\tomva\\onedrive\\ku leuven\\master theory and research\\internship\\rtnet\\venv\\lib\\site-packages (from torch>=0.4.0->pyro-ppl==0.2.1) (4.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tomva\\onedrive\\ku leuven\\master theory and research\\internship\\rtnet\\venv\\lib\\site-packages (from jinja2->torch>=0.4.0->pyro-ppl==0.2.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tomva\\onedrive\\ku leuven\\master theory and research\\internship\\rtnet\\venv\\lib\\site-packages (from sympy->torch>=0.4.0->pyro-ppl==0.2.1) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\tomva\\onedrive\\ku leuven\\master theory and research\\internship\\rtnet\\venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyro-ppl==0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "N6jMxsF_xVp4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import Normal, Categorical\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7rmEFaPL30Sh",
    "outputId": "ab3d4dd6-0d22-4f94-a9d2-84a62537a76b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0DJT0xeAqXL"
   },
   "source": [
    "**Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tTRmzMB4yMxj",
    "outputId": "1d307cc1-8927-4dde-b0ec-a52edd0e49b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1131)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to mnist-data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist-data/MNIST\\raw\\train-images-idx3-ubyte.gz to mnist-data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1131)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to mnist-data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist-data/MNIST\\raw\\train-labels-idx1-ubyte.gz to mnist-data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1131)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to mnist-data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist-data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to mnist-data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1131)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to mnist-data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist-data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to mnist-data/MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "AlexTransform = transforms.Compose([\n",
    "    transforms.Resize((227, 227)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('mnist-data/', train=True, download=True, transform=AlexTransform),\n",
    "        batch_size=500, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('mnist-data/', train=False, download=True, transform=AlexTransform),\n",
    "        batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyqaSFcs3Udj"
   },
   "source": [
    "**Define the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4QqsElxSF2F"
   },
   "source": [
    "AlexNet structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XNBayOSDyyn9"
   },
   "outputs": [],
   "source": [
    "# AlexNet\n",
    "class alexnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=96, kernel_size=11, stride=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, 3, 1, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 384, 3, 1, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.conv5(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        out = F.relu(self.fc1(out))  # 256*6*6 -> 4096\n",
    "        out = F.dropout(out, 0.5)\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = F.dropout(out, 0.5)\n",
    "        out = self.fc3(out)\n",
    "        # out = F.log_softmax(out, dim=1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "l-94O5iW367h"
   },
   "outputs": [],
   "source": [
    "# Define Hyper parameters\n",
    "img_size = 28 * 28\n",
    "hidden_layer_size = 1024\n",
    "num_classes = 10\n",
    "net = alexnet().to(device)\n",
    "# softmax\n",
    "log_softmax = nn.LogSoftmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COLZgZNISK4g"
   },
   "source": [
    "Model function for pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "liRRzYaD4Ejg"
   },
   "outputs": [],
   "source": [
    "def model(x_data, y_data):\n",
    "\n",
    "    convLayer1_w = Normal(loc=torch.ones_like(net.conv1[0].weight), scale=torch.ones_like(net.conv1[0].weight))\n",
    "    convLayer1_b = Normal(loc=torch.ones_like(net.conv1[0].bias), scale=torch.ones_like(net.conv1[0].bias))\n",
    "\n",
    "    convLayer2_w = Normal(loc=torch.ones_like(net.conv2[0].weight), scale=torch.ones_like(net.conv2[0].weight))\n",
    "    convLayer2_b = Normal(loc=torch.ones_like(net.conv2[0].bias), scale=torch.ones_like(net.conv2[0].bias))\n",
    "\n",
    "    convLayer3_w = Normal(loc=torch.ones_like(net.conv3[0].weight), scale=torch.ones_like(net.conv3[0].weight))\n",
    "    convLayer3_b = Normal(loc=torch.ones_like(net.conv3[0].bias), scale=torch.ones_like(net.conv3[0].bias))\n",
    "\n",
    "    convLayer4_w = Normal(loc=torch.ones_like(net.conv4[0].weight), scale=torch.ones_like(net.conv4[0].weight))\n",
    "    convLayer4_b = Normal(loc=torch.ones_like(net.conv4[0].bias), scale=torch.ones_like(net.conv4[0].bias))\n",
    "\n",
    "    convLayer5_w = Normal(loc=torch.ones_like(net.conv5[0].weight), scale=torch.ones_like(net.conv5[0].weight))\n",
    "    convLayer5_b = Normal(loc=torch.ones_like(net.conv5[0].bias), scale=torch.ones_like(net.conv5[0].bias))\n",
    "\n",
    "    fc1Layer_w = Normal(loc=torch.ones_like(net.fc1.weight), scale=torch.ones_like(net.fc1.weight))\n",
    "    fc1Layer_b = Normal(loc=torch.ones_like(net.fc1.bias), scale=torch.ones_like(net.fc1.bias))\n",
    "\n",
    "    fc2Layer_w = Normal(loc=torch.ones_like(net.fc2.weight), scale=torch.ones_like(net.fc2.weight))\n",
    "    fc2Layer_b = Normal(loc=torch.ones_like(net.fc2.bias), scale=torch.ones_like(net.fc2.bias))\n",
    "\n",
    "    fc3Layer_w = Normal(loc=torch.ones_like(net.fc3.weight), scale=torch.ones_like(net.fc3.weight))\n",
    "    fc3Layer_b = Normal(loc=torch.ones_like(net.fc3.bias), scale=torch.ones_like(net.fc3.bias))\n",
    "\n",
    "    priors = {'conv1[0].weight': convLayer1_w,\n",
    "              'conv1[0].bias': convLayer1_b,\n",
    "              'conv2[0].weight': convLayer2_w,\n",
    "              'conv2[0].bias': convLayer2_b,\n",
    "              'conv3[0].weight': convLayer3_w,\n",
    "              'conv3[0].bias': convLayer3_b,\n",
    "              'conv4[0].weight': convLayer4_w,\n",
    "              'conv4[0].bias': convLayer4_b,\n",
    "              'conv5[0].weight': convLayer5_w,\n",
    "              'conv5[0].bias': convLayer5_b,\n",
    "              'fc1.weight': fc1Layer_w,\n",
    "              'fc1.bias': fc1Layer_b,\n",
    "              'fc2.weight': fc2Layer_w,\n",
    "              'fc2.bias': fc2Layer_b,\n",
    "              'fc3.weight': fc3Layer_w,\n",
    "              'fc3.bias': fc3Layer_b}\n",
    "\n",
    "    # lift module parameters to random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module(\"module\", net, priors)\n",
    "    # sample a regressor (which also samples w and b)\n",
    "    lifted_reg_model = lifted_module()\n",
    "\n",
    "    lhat = log_softmax(lifted_reg_model(x_data))\n",
    "\n",
    "    pyro.sample(\"obs\", Categorical(logits=lhat), obs=y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hX4OnUndST19"
   },
   "source": [
    "Guide function for pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "So18-_z_4Hv3"
   },
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "\n",
    "def guide(x_data, y_data):\n",
    "\n",
    "    # First layer weight distribution priors\n",
    "    convLayer1w_mu    = torch.randn_like(net.conv1[0].weight)\n",
    "    convLayer1w_sigma = torch.randn_like(net.conv1[0].weight)\n",
    "    convLayer1w_mu_param    = pyro.param(\"convLayer1w_mu\", convLayer1w_mu)\n",
    "    convLayer1w_sigma_param = softplus(pyro.param(\"convLayer1w_sigma\", convLayer1w_sigma))\n",
    "    convLayer1_w = Normal(loc=convLayer1w_mu_param, scale=convLayer1w_sigma_param)\n",
    "\n",
    "    # First layer bias distribution priors\n",
    "    convLayer1b_mu    = torch.randn_like(net.conv1[0].bias)\n",
    "    convLayer1b_sigma = torch.randn_like(net.conv1[0].bias)\n",
    "    convLayer1b_mu_param    = pyro.param(\"convLayer1b_mu\", convLayer1b_mu)\n",
    "    convLayer1b_sigma_param = softplus(pyro.param(\"convLayer1b_sigma\", convLayer1b_sigma))\n",
    "    convLayer1_b = Normal(loc=convLayer1b_mu_param, scale=convLayer1b_sigma_param)\n",
    "\n",
    "    # Second layer weight distribution priors\n",
    "    convLayer2w_mu    = torch.randn_like(net.conv2[0].weight)\n",
    "    convLayer2w_sigma = torch.randn_like(net.conv2[0].weight)\n",
    "    convLayer2w_mu_param    = pyro.param(\"convLayer2w_mu\", convLayer2w_mu)\n",
    "    convLayer2w_sigma_param = softplus(pyro.param(\"convLayer2w_sigma\", convLayer2w_sigma))\n",
    "    convLayer2_w = Normal(loc=convLayer2w_mu_param, scale=convLayer2w_sigma_param)\n",
    "\n",
    "    # Second layer bias distribution priors\n",
    "    convLayer2b_mu    = torch.randn_like(net.conv2[0].bias)\n",
    "    convLayer2b_sigma = torch.randn_like(net.conv2[0].bias)\n",
    "    convLayer2b_mu_param    = pyro.param(\"convLayer2b_mu\", convLayer2b_mu)\n",
    "    convLayer2b_sigma_param = softplus(pyro.param(\"convLayer2b_sigma\", convLayer2b_sigma))\n",
    "    convLayer2_b = Normal(loc=convLayer2b_mu_param, scale=convLayer2b_sigma_param)\n",
    "\n",
    "    # Third layer weight distribution priors\n",
    "    convLayer3w_mu    = torch.randn_like(net.conv3[0].weight)\n",
    "    convLayer3w_sigma = torch.randn_like(net.conv3[0].weight)\n",
    "    convLayer3w_mu_param    = pyro.param(\"convLayer3w_mu\", convLayer3w_mu)\n",
    "    convLayer3w_sigma_param = softplus(pyro.param(\"convLayer3w_sigma\", convLayer3w_sigma))\n",
    "    convLayer3_w = Normal(loc=convLayer3w_mu_param, scale=convLayer3w_sigma_param)\n",
    "\n",
    "    # Third layer bias distribution priors\n",
    "    convLayer3b_mu    = torch.randn_like(net.conv3[0].bias)\n",
    "    convLayer3b_sigma = torch.randn_like(net.conv3[0].bias)\n",
    "    convLayer3b_mu_param    = pyro.param(\"convLayer3b_mu\", convLayer3b_mu)\n",
    "    convLayer3b_sigma_param = softplus(pyro.param(\"convLayer3b_sigma\", convLayer3b_sigma))\n",
    "    convLayer3_b = Normal(loc=convLayer3b_mu_param, scale=convLayer3b_sigma_param)\n",
    "\n",
    "    # Fourth layer weight distribution priors\n",
    "    convLayer4w_mu    = torch.randn_like(net.conv4[0].weight)\n",
    "    convLayer4w_sigma = torch.randn_like(net.conv4[0].weight)\n",
    "    convLayer4w_mu_param    = pyro.param(\"convLayer4w_mu\", convLayer4w_mu)\n",
    "    convLayer4w_sigma_param = softplus(pyro.param(\"convLayer4w_sigma\", convLayer4w_sigma))\n",
    "    convLayer4_w = Normal(loc=convLayer4w_mu_param, scale=convLayer4w_sigma_param)\n",
    "\n",
    "    # Fourth layer bias distribution priors\n",
    "    convLayer4b_mu    = torch.randn_like(net.conv4[0].bias)\n",
    "    convLayer4b_sigma = torch.randn_like(net.conv4[0].bias)\n",
    "    convLayer4b_mu_param    = pyro.param(\"convLayer4b_mu\", convLayer4b_mu)\n",
    "    convLayer4b_sigma_param = softplus(pyro.param(\"convLayer4b_sigma\", convLayer4b_sigma))\n",
    "    convLayer4_b = Normal(loc=convLayer4b_mu_param, scale=convLayer4b_sigma_param)\n",
    "\n",
    "    # Fifth layer weight distribution priors\n",
    "    convLayer5w_mu    = torch.randn_like(net.conv5[0].weight)\n",
    "    convLayer5w_sigma = torch.randn_like(net.conv5[0].weight)\n",
    "    convLayer5w_mu_param    = pyro.param(\"convLayer5w_mu\", convLayer5w_mu)\n",
    "    convLayer5w_sigma_param = softplus(pyro.param(\"convLayer5w_sigma\", convLayer5w_sigma))\n",
    "    convLayer5_w = Normal(loc=convLayer5w_mu_param, scale=convLayer5w_sigma_param)\n",
    "\n",
    "    # Fifth layer bias distribution priors\n",
    "    convLayer5b_mu    = torch.randn_like(net.conv5[0].bias)\n",
    "    convLayer5b_sigma = torch.randn_like(net.conv5[0].bias)\n",
    "    convLayer5b_mu_param    = pyro.param(\"convLayer5b_mu\", convLayer5b_mu)\n",
    "    convLayer5b_sigma_param = softplus(pyro.param(\"convLayer5b_sigma\", convLayer5b_sigma))\n",
    "    convLayer5_b = Normal(loc=convLayer5b_mu_param, scale=convLayer5b_sigma_param)\n",
    "\n",
    "    # First fully connected layer weight distribution priors\n",
    "    fc1w_mu = torch.randn_like(net.fc1.weight)\n",
    "    fc1w_sigma = torch.randn_like(net.fc1.weight)\n",
    "    fc1w_mu_param = pyro.param(\"fc1w_mu\", fc1w_mu)\n",
    "    fc1w_sigma_param = softplus(pyro.param(\"fc1w_sigma\", fc1w_sigma))\n",
    "    fc1Layer_w = Normal(loc=fc1w_mu_param, scale=fc1w_sigma_param).independent(1)\n",
    "\n",
    "    # First fully connected layer bias distribution priors\n",
    "    fc1b_mu = torch.randn_like(net.fc1.bias)\n",
    "    fc1b_sigma = torch.randn_like(net.fc1.bias)\n",
    "    fc1b_mu_param = pyro.param(\"fc1b_mu\", fc1b_mu)\n",
    "    fc1b_sigma_param = softplus(pyro.param(\"fc1b_sigma\", fc1b_sigma))\n",
    "    fc1Layer_b = Normal(loc=fc1b_mu_param, scale=fc1b_sigma_param)\n",
    "\n",
    "    # Second fully connected layer weight distribution priors\n",
    "    fc2w_mu = torch.randn_like(net.fc2.weight)\n",
    "    fc2w_sigma = torch.randn_like(net.fc2.weight)\n",
    "    fc2w_mu_param = pyro.param(\"fc2w_mu\", fc2w_mu)\n",
    "    fc2w_sigma_param = softplus(pyro.param(\"fc2w_sigma\", fc2w_sigma))\n",
    "    fc2Layer_w = Normal(loc=fc2w_mu_param, scale=fc2w_sigma_param).independent(1)\n",
    "\n",
    "    # Second fully connected layer bias distribution priors\n",
    "    fc2b_mu = torch.randn_like(net.fc2.bias)\n",
    "    fc2b_sigma = torch.randn_like(net.fc2.bias)\n",
    "    fc2b_mu_param = pyro.param(\"fc2b_mu\", fc2b_mu)\n",
    "    fc2b_sigma_param = softplus(pyro.param(\"fc2b_sigma\", fc2b_sigma))\n",
    "    fc2Layer_b = Normal(loc=fc2b_mu_param, scale=fc2b_sigma_param)\n",
    "\n",
    "    # Third fully connected layer weight distribution priors\n",
    "    fc3w_mu = torch.randn_like(net.fc3.weight)\n",
    "    fc3w_sigma = torch.randn_like(net.fc3.weight)\n",
    "    fc3w_mu_param = pyro.param(\"fc3w_mu\", fc3w_mu)\n",
    "    fc3w_sigma_param = softplus(pyro.param(\"fc3w_sigma\", fc3w_sigma))\n",
    "    fc3Layer_w = Normal(loc=fc3w_mu_param, scale=fc3w_sigma_param).independent(1)\n",
    "\n",
    "    # Third fully connected layer bias distribution priors\n",
    "    fc3b_mu = torch.randn_like(net.fc3.bias)\n",
    "    fc3b_sigma = torch.randn_like(net.fc3.bias)\n",
    "    fc3b_mu_param = pyro.param(\"fc3b_mu\", fc3b_mu)\n",
    "    fc3b_sigma_param = softplus(pyro.param(\"fc3b_sigma\", fc3b_sigma))\n",
    "    fc3Layer_b = Normal(loc=fc3b_mu_param, scale=fc3b_sigma_param)\n",
    "\n",
    "    priors = {'conv1[0].weight': convLayer1_w,\n",
    "              'conv1[0].bias': convLayer1_b,\n",
    "              'conv2[0].weight': convLayer2_w,\n",
    "              'conv2[0].bias': convLayer2_b,\n",
    "              'conv3[0].weight': convLayer3_w,\n",
    "              'conv3[0].bias': convLayer3_b,\n",
    "              'conv4[0].weight': convLayer4_w,\n",
    "              'conv4[0].bias': convLayer4_b,\n",
    "              'conv5[0].weight': convLayer5_w,\n",
    "              'conv5[0].bias': convLayer5_b,\n",
    "              'fc1.weight': fc1Layer_w,\n",
    "              'fc1.bias': fc1Layer_b,\n",
    "              'fc2.weight': fc2Layer_w,\n",
    "              'fc2.bias': fc2Layer_b,\n",
    "              'fc3.weight': fc3Layer_w,\n",
    "              'fc3.bias': fc3Layer_b}\n",
    "\n",
    "    lifted_module = pyro.random_module(\"module\", net, priors)\n",
    "\n",
    "    return lifted_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wO3LecLVzsux"
   },
   "source": [
    "**Training a model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uO7gpZVazwz3"
   },
   "outputs": [],
   "source": [
    "optim = Adam({'lr': 0.01})\n",
    "svi = SVI(model, guide, optim, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-t2jl0MH3kow"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_id, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# calculate the loss and take a gradient step\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43msvi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m normalizer_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m     10\u001b[0m total_epoch_loss_train \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m normalizer_train\n",
      "File \u001b[1;32mc:\\Users\\tomva\\OneDrive\\KU Leuven\\Master Theory and Research\\Internship\\RTNet\\venv\\lib\\site-packages\\pyro\\infer\\svi.py:75\u001b[0m, in \u001b[0;36mSVI.step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# get loss and compute gradients\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m poutine\u001b[38;5;241m.\u001b[39mtrace(param_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m param_capture:\n\u001b[1;32m---> 75\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_and_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munconstrained()\n\u001b[0;32m     78\u001b[0m              \u001b[38;5;28;01mfor\u001b[39;00m site \u001b[38;5;129;01min\u001b[39;00m param_capture\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# actually perform gradient steps\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# torch.optim objects gets instantiated for any params that haven't been seen yet\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tomva\\OneDrive\\KU Leuven\\Master Theory and Research\\Internship\\RTNet\\venv\\lib\\site-packages\\pyro\\infer\\trace_elbo.py:107\u001b[0m, in \u001b[0;36mTrace_ELBO.loss_and_grads\u001b[1;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m elbo \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# grab a trace from the generator\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_trace, guide_trace \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_traces(model, guide, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    108\u001b[0m     elbo_particle \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    109\u001b[0m     surrogate_elbo_particle \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\tomva\\OneDrive\\KU Leuven\\Master Theory and Research\\Internship\\RTNet\\venv\\lib\\site-packages\\pyro\\infer\\trace_elbo.py:55\u001b[0m, in \u001b[0;36mTrace_ELBO._get_traces\u001b[1;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_particles):\n\u001b[0;32m     54\u001b[0m     guide_trace \u001b[38;5;241m=\u001b[39m poutine\u001b[38;5;241m.\u001b[39mtrace(guide)\u001b[38;5;241m.\u001b[39mget_trace(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 55\u001b[0m     model_trace \u001b[38;5;241m=\u001b[39m \u001b[43mpoutine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoutine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguide_trace\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_validation_enabled():\n\u001b[0;32m     57\u001b[0m         check_model_guide_match(model_trace, guide_trace)\n",
      "File \u001b[1;32mc:\\Users\\tomva\\OneDrive\\KU Leuven\\Master Theory and Research\\Internship\\RTNet\\venv\\lib\\site-packages\\pyro\\poutine\\trace_messenger.py:202\u001b[0m, in \u001b[0;36mTraceHandler.get_trace\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    195\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m    :returns: data structure\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m    :rtype: pyro.poutine.Trace\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m    Calls this poutine and returns its trace instead of the function's return value.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsngr\u001b[38;5;241m.\u001b[39mget_trace()\n",
      "File \u001b[1;32mc:\\Users\\tomva\\OneDrive\\KU Leuven\\Master Theory and Research\\Internship\\RTNet\\venv\\lib\\site-packages\\pyro\\poutine\\trace_messenger.py:186\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsngr:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsngr\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_INPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    184\u001b[0m                               name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_INPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    185\u001b[0m                               args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 186\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsngr\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_RETURN\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_RETURN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, value\u001b[38;5;241m=\u001b[39mret)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\tomva\\OneDrive\\KU Leuven\\Master Theory and Research\\Internship\\RTNet\\venv\\lib\\site-packages\\pyro\\poutine\\messenger.py:27\u001b[0m, in \u001b[0;36mMessenger.__call__.<locals>._wraps\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wraps\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 51\u001b[0m, in \u001b[0;36mmodel\u001b[1;34m(x_data, y_data)\u001b[0m\n\u001b[0;32m     47\u001b[0m lifted_reg_model \u001b[38;5;241m=\u001b[39m lifted_module()\n\u001b[0;32m     49\u001b[0m lhat \u001b[38;5;241m=\u001b[39m log_softmax(lifted_reg_model(x_data))\n\u001b[1;32m---> 51\u001b[0m pyro\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlhat\u001b[49m\u001b[43m)\u001b[49m, obs\u001b[38;5;241m=\u001b[39my_data)\n",
      "File \u001b[1;32mc:\\Users\\tomva\\OneDrive\\KU Leuven\\Master Theory and Research\\Internship\\RTNet\\venv\\lib\\site-packages\\torch\\distributions\\categorical.py:71\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     68\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[0;32m     70\u001b[0m )\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tomva\\OneDrive\\KU Leuven\\Master Theory and Research\\Internship\\RTNet\\venv\\lib\\site-packages\\torch\\distributions\\distribution.py:69\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     67\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[0;32m     68\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m---> 69\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m     70\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     71\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m             )\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_iterations = 5\n",
    "loss = 0\n",
    "\n",
    "for j in range(num_iterations):\n",
    "    loss = 0\n",
    "    for batch_id, data in enumerate(train_loader):\n",
    "        # calculate the loss and take a gradient step\n",
    "        loss += svi.step(data[0].to(device), data[1].to(device))\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = loss / normalizer_train\n",
    "\n",
    "    print(\"Epoch \", j, \" Loss \", total_epoch_loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykaKfXV-3746"
   },
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JW2yCU32393A"
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "num_samples = 3\n",
    "def predict(x):\n",
    "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
    "    yhats = [model(x).data for model in sampled_models]\n",
    "    mean = torch.mean(torch.stack(yhats), 0)\n",
    "    return torch.argmax(mean, dim=1)\n",
    "\n",
    "\n",
    "print('Prediction when network is forced to predict')\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "all_labels = []\n",
    "all_predicted = []\n",
    "for j, data in enumerate(test_loader):\n",
    "    images, labels = data\n",
    "    predicted = predict(images.to(device))\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels.to(device)).sum().item()\n",
    "    all_labels.append(labels)\n",
    "    all_predicted.append(predicted)\n",
    "print(\"accuracy: %d %%\" % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9zlHfh14IDz"
   },
   "source": [
    "**Save the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoK5Rpba4LDw"
   },
   "outputs": [],
   "source": [
    "path = 'INSERT PATH HERE'\n",
    "model_num = '01'\n",
    "torch.save({\"model\" : net.state_dict(), \"guide\" : guide}, path + \"model_\" + model_num + \".pt\")\n",
    "pyro.get_param_store().save(path + \"model_\" + model_num + \"_params.pt\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
